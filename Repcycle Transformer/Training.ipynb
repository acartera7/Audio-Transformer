{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Description: This script trains an AST whose input has been modified to take audio insteasd of patches of images \n",
    "# Original code is based off a tutorial by Brian Pulfer\n",
    "# https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "# Andrei Cartera -- Mar 2025\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "import CustomSpeechCommands_Repcycle as SpeechCommands\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrei\\Documents\\GitHub\\Audio-Transformer\\Repcycle Transformer\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model path: models/(2025-05-08)New_ATmodel_32SEG_64VEC_E50_8_4_B64_H32.pth\n"
     ]
    }
   ],
   "source": [
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Hyperparameters\n",
    "N_SEGMENTS = 32\n",
    "REPC_VEC_SIZE = 64\n",
    "\n",
    "EPOCHS = 50\n",
    "N_HEADS = 8\n",
    "N_ENCODERS = 4\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 32\n",
    "DROPOUT = 0.15\n",
    "ACTIVATION=\"gelu\"\n",
    "LR = 0.0009\n",
    "\n",
    "today = datetime.date.today()\n",
    "\n",
    "MODEL_PATH = f'models/({today})New_ATmodel_{N_SEGMENTS}SEG_{REPC_VEC_SIZE}VEC_E{EPOCHS}_{N_HEADS}_{N_ENCODERS}_B{BATCH_SIZE}_H{HIDDEN_DIM}.pth'\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "  result = torch.ones(sequence_length, d)\n",
    "  for i in range(sequence_length):\n",
    "    for j in range(d):\n",
    "      result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorizedMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads):\n",
    "    super(VectorizedMSA, self).__init__()\n",
    "    assert d % n_heads == 0, f\"Dimension {d} not divisible by {n_heads}\"\n",
    "    self.n_heads = n_heads\n",
    "    self.d = d\n",
    "    self.d_head = d // n_heads\n",
    "    \n",
    "    # One linear layer each for q, k, v to project from d -> d\n",
    "    self.q_linear = nn.Linear(d, d)\n",
    "    self.k_linear = nn.Linear(d, d)\n",
    "    self.v_linear = nn.Linear(d, d)\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # x: (batch, seq_len, d)\n",
    "    batch_size, seq_len, d = x.size()\n",
    "    \n",
    "    # Project to queries, keys, values\n",
    "    q = self.q_linear(x)  # (batch, seq_len, d)\n",
    "    k = self.k_linear(x)\n",
    "    v = self.v_linear(x)\n",
    "    \n",
    "    # Reshape to (batch, seq_len, n_heads, d_head) then transpose -> (batch, n_heads, seq_len, d_head)\n",
    "    q = q.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "    k = k.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "    v = v.view(batch_size, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "    \n",
    "    # Compute scaled dot-product attention\n",
    "    attn_scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)  # (batch, n_heads, seq_len, seq_len)\n",
    "    attn_probs = self.softmax(attn_scores)\n",
    "    attn_output = torch.matmul(attn_probs, v)  # (batch, n_heads, seq_len, d_head)\n",
    "    \n",
    "    # Concatenate heads: (batch, seq_len, n_heads, d_head) -> (batch, seq_len, d)\n",
    "    attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, seq_len, d)\n",
    "    return attn_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads):\n",
    "    super(NewMSA, self).__init__()\n",
    "    self.multihead_attn = nn.MultiheadAttention(embed_dim=d, num_heads=n_heads, dropout=DROPOUT)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    # x: (batch, seq_len, d)\n",
    "    # Transpose to (seq_len, batch, d) as required by nn.MultiheadAttention\n",
    "    x_t = x.transpose(0, 1)\n",
    "    # Compute self-attention with queries, keys and values set to x_t\n",
    "    attn_output, _ = self.multihead_attn(x_t, x_t, x_t)\n",
    "    # Transpose back to (batch, seq_len, d)\n",
    "    return attn_output.transpose(0, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads=2):\n",
    "    super(MyMSA, self).__init__()\n",
    "    self.d = d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "    d_head = int(d / n_heads)\n",
    "    self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.d_head = d_head\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    # Sequences has shape (N, seq_length, token_dim)\n",
    "    # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "    # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "      seq_result = []\n",
    "      for head in range(self.n_heads):\n",
    "        q_mapping = self.q_mappings[head]\n",
    "        k_mapping = self.k_mappings[head]\n",
    "        v_mapping = self.v_mappings[head]\n",
    "\n",
    "        seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "        q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "        attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "        seq_result.append(attention @ v)\n",
    "      result.append(torch.hstack(seq_result))\n",
    "    return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransformerBlock(nn.Module):\n",
    "  def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "    super(AudioTransformerBlock, self).__init__()\n",
    "    self.hidden_d = hidden_d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(hidden_d)\n",
    "    self.mhsa = NewMSA(hidden_d, n_heads)  # Updated MSA module\n",
    "    self.norm2 = nn.LayerNorm(hidden_d)\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mhsa(self.norm1(x))\n",
    "    out = out + self.mlp(self.norm2(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransformer(nn.Module):\n",
    "  def __init__(self, n_segments, repc_vec_size , n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "    # Super constructor\n",
    "    super(AudioTransformer, self).__init__()\n",
    "    \n",
    "    # Attributes\n",
    "    self.n_segments = n_segments\n",
    "    self.repc_vec_size  = repc_vec_size \n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_heads = n_heads\n",
    "    self.hidden_d = hidden_d\n",
    "    \n",
    "    # 1) Linear mapper\n",
    "    self.input_d = repc_vec_size\n",
    "    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "    \n",
    "    # 2) Learnable classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "    \n",
    "    # 3) Positional Embedding\n",
    "    self.register_buffer(\n",
    "      'positional_embeddings',\n",
    "      get_positional_embeddings(n_segments + 1, hidden_d),\n",
    "      persistent=False\n",
    "    )\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([AudioTransformerBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "    \n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self.hidden_d, out_d),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, audio):\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    tokens = self.linear_mapper(audio)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.cat((self.class_token.expand(audio.shape[0], 1, -1), tokens), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = tokens + self.positional_embeddings.repeat(audio.shape[0], 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "        \n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    \n",
    "    return self.mlp(out) # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "  # Loading data\n",
    "\n",
    "  \n",
    "\n",
    "  print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "  model = AudioTransformer(N_SEGMENTS, REPC_VEC_SIZE, N_ENCODERS, HIDDEN_DIM, N_HEADS, NUM_CLASSES).to(device)\n",
    "\n",
    "  train_set = SpeechCommands.CustomSpeechCommandsDataset_Repcycle(\"../custom_speech_commands\", n_segments=N_SEGMENTS, shuffle=False, vec_size=REPC_VEC_SIZE, divisor=BATCH_SIZE)\n",
    "  \n",
    "  train_loader = DataLoader(train_set, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "  # Defining model and training options\n",
    "\n",
    "  # Training loop\n",
    "  optimizer = Adam(model.parameters(), lr=LR)\n",
    "  criterion = CrossEntropyLoss()\n",
    "\n",
    "  model.train()  # Set the model to training mode                                     \n",
    "  for epoch in trange(EPOCHS, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "      x, y = batch\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item() * x.size(0)\n",
    "      \n",
    "    train_loss /= len(train_loader.dataset) \n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} loss: {train_loss:.2f}, LR: {current_lr}\")\n",
    "  \n",
    "  torch.save(model.state_dict(), MODEL_PATH)\n",
    "  print(f\"Model saved as {MODEL_PATH}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "  model = AudioTransformer(N_SEGMENTS, REPC_VEC_SIZE, N_ENCODERS, HIDDEN_DIM, N_HEADS, NUM_CLASSES).to(device)\n",
    "  model.load_state_dict(torch.load(MODEL_PATH, weights_only=True))\n",
    "  print(f\"Model loaded from {MODEL_PATH}\")\n",
    "  \n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  \n",
    "  test_set = SpeechCommands.CustomSpeechCommandsDataset_Repcycle(\"../custom_speech_commands\", n_segments=N_SEGMENTS, subset=\"testing\", shuffle=True, vec_size=REPC_VEC_SIZE, divisor=BATCH_SIZE)\n",
    "  test_loader = DataLoader(test_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "\n",
    "  criterion = CrossEntropyLoss()\n",
    "\n",
    "  # Test loop\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "      correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "      total += len(x)\n",
    "      \n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:0 (NVIDIA GeForce RTX 4060)\n",
      "Balanced dataset to 3264 samples per label.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67dfc962edd40d79435d6cf21acf20f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7083c59a12648389548d2fa985debe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 in training:   0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 loss: 2.37, LR: 0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99df801d02954c7a9531d4b586412e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2 in training:   0%|          | 0/510 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  train()\n",
    "  test() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
