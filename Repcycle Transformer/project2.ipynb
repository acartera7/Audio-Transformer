{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "### This model grabs Pitch and Timbre\n",
    "\n",
    "#### Project Progress :\n",
    "\n",
    "1. ~~find zero-crossings~~\n",
    "2. ~~find cycles~~\n",
    "3. ~~choose representative cycles based on biases~~\n",
    "4. form a model with bcoeffs for each representative cycle\n",
    "5. interpolate between each cycle using bcoeffs for amplitude shape and glissando for pitch\n",
    "6. test accuracy to original sound\n",
    "7. Tokenize and train model\n",
    "8. Profit?\n",
    "   \n",
    "#### Ideas\n",
    "\n",
    "Test with no interpolation between the representative cycles or by sample-and-hold the cycles for a duration\n",
    "\n",
    "What if instead of tokenizing the b-spline coefficients we use the overtones or harmonics of each cycle? At the end of the day, we know that a neural network can distinguish pitch visually.\n",
    "\n",
    "What if we tokenize the information of the differencs in the b-splines\n",
    "\n",
    "To better find cycles: group cycles by similarity by using a dot product, larger dot products are much more similar. Find one from the largest group as your repcycle. Make a Union of the groups to find the group whose cycles covers the whole segment.\n",
    "\n",
    "Compare cycles of next segment with the repcycle of the previous \n",
    "\n",
    "If the segment has 30+ cycles in its segment, it might be noise therfore bypass the 1st 3rd bias \n",
    "\n",
    "Instead of using Bspline coefficients, just use vectorized versions\n",
    "\n",
    "Make custom FFT that limits bins to only range we need, with optential peek ahead to see dominant frequencies over\n",
    "\n",
    "Todo RECONSTRUCT SAMPLE-AND-HOLD VECTORIZED\n",
    "\n",
    "TODO create commandline process to make repcycles and of the dataset based on parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Testing for Pytorch Audio prcessing\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Andrei Cartera \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#from pathlib import Path\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Testing for Pytorch Audio prcessing\n",
    "# Andrei Cartera \n",
    "import os, sys\n",
    "import numpy as np\n",
    "#from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn.functional as nnF\n",
    "#from IPython.display import Image\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torchaudio \n",
    "\n",
    "# Install soundfile and ffmpeg-python if not already installed\n",
    "# pip install soundfile\n",
    "# pip install ffmpeg-python\n",
    "\n",
    "print(torch.__version__)\n",
    "print(str(torchaudio.list_audio_backends()))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = \"./audio/\"\n",
    "FILE_NAME = \"one.wav\"\n",
    "AUDIO_SIZE = 16000\n",
    "SEGMENT_LENGTH = 500\n",
    "SEGMENT_NUM = 5\n",
    "NOISE_FACTOR = 0.65\n",
    "CYCLE_EPSILON = 30 # make it percentage wise 25%\n",
    "FFT_SIZE = 1024\n",
    "\n",
    "REPC_VEC_SIZE = 64\n",
    "FRONT_VEC_SIZE = 40\n",
    "Z_THRESHOLD = 1.0\n",
    "DIFF_THRESHOLD = 0.3\n",
    "\n",
    "OPEN_WAV = True\n",
    "OPEN_FFT = False\n",
    "OPEN_REPC = True\n",
    "SHOW_ZEROS = True\n",
    "\n",
    "WAV_FIG_SIZE = (50, 10)\n",
    "FFT_FIG_SIZE = (40, 10)\n",
    "F0_FREQ_RANGE = (50,250)\n",
    "\n",
    "# how much to conisder frequency, loudness, or middleness, higher values means more adherence\n",
    "RMS_WEIGHT    = 1 # RMS loudness\n",
    "FREQ_WEIGHT   = 1 # Frequency \n",
    "MID_WEIGHT    = 2 # Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "LOW_F0_INDEX = math.floor(FFT_SIZE/AUDIO_SIZE * F0_FREQ_RANGE[0])\n",
    "HIGH_F0_INDEX = math.floor(FFT_SIZE/AUDIO_SIZE * F0_FREQ_RANGE[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_value(waveform_arr, location:float):\n",
    "    # Ensure the location is within the bounds of the array\n",
    "  if location < 0 or location > len(waveform_arr) - 1:\n",
    "    raise ValueError(\"Location is out of bounds.\")\n",
    "\n",
    "  # Define the indices for interpolation\n",
    "  indices = np.arange(len(waveform_arr))\n",
    "\n",
    "  # Use numpy's interp function to get the interpolated value\n",
    "  return np.interp(location, indices, waveform_arr)\n",
    "\n",
    "def vectorize(waveform_arr, n:int):\n",
    "  indices = np.linspace(0, len(waveform_arr) - 1, n, dtype=int)\n",
    "  return waveform_arr[indices]\n",
    "\n",
    "def vectorize_f(waveform_arr, n:int):\n",
    "  if len(waveform_arr) == n:\n",
    "    return waveform_arr\n",
    "  if len(waveform_arr) < n:\n",
    "    # resample the waveform to fit the desired size\n",
    "    return resample(waveform_arr, n)\n",
    "    # pad with zeros\n",
    "    #return np.pad(waveform_arr.numpy(), (0,n - len(waveform_arr)))\n",
    "  \n",
    "  sample_points = np.linspace(0, len(waveform_arr) - 1, n)\n",
    "  return np.array([wav_value(waveform_arr, point) for point in sample_points])\n",
    "\n",
    "def resample(waveform_arr, n:int):\n",
    "  resampled_waveform = np.zeros(n)\n",
    "  sample_points = np.linspace(0, len(waveform_arr) - 1, n)\n",
    "  for i, point in enumerate(sample_points):\n",
    "    resampled_waveform[i] = wav_value(waveform_arr, point)\n",
    "  return resampled_waveform\n",
    "\n",
    "def bias_front(waveform_arr):\n",
    "  front = (len(waveform_arr) - 1) // 3\n",
    "  #front = int((len(waveform_arr) - 1) // (f0//(F0_FREQ_RANGE[0]//2)))\n",
    "  #adds up 1st 3rd of the\n",
    "  return np.sum(waveform_arr[:front])\n",
    "\n",
    "def bias_front_abs(waveform_arr):\n",
    "  front = (len(waveform_arr) - 1) // 3\n",
    "  #front = int((len(waveform_arr) - 1) // (f0//(F0_FREQ_RANGE[0]//2)))\n",
    "  #adds up 1st 3rd of the\n",
    "  return np.sum(abs(waveform_arr[:front]))\n",
    "  \n",
    "# gets k largest elements from a list along with its indices \n",
    "def klargest_with_indices(arr, k, range=None): \n",
    "  heap = []\n",
    "  for i, val in enumerate(arr):\n",
    "      if range and not (range[0] <= i <= range[1]):\n",
    "        continue\n",
    "      heap.append((i,val))\n",
    "  return heapq.nlargest(k, heap, key=lambda x: x[1])\n",
    "\n",
    "def calculate_rms(audio):\n",
    "  return np.sqrt(np.mean(audio**2))\n",
    "\n",
    "def find_factor_pairs(n):\n",
    "  factor_pairs = []\n",
    "  for i in range(1, int(n**0.5) + 1):\n",
    "    if n % i == 0:\n",
    "      factor_pairs.append((i, int(n // i)))\n",
    "  return factor_pairs\n",
    "\n",
    "#def group_tuples(tuples, range_size):\n",
    "#  # Sort tuples based on the second element\n",
    "#  tuples_sorted = sorted(tuples, key=lambda x: x[1])  \n",
    "#  \n",
    "#  # Group tuples based on the range_size\n",
    "#  groups = []\n",
    "#  current_group = []\n",
    "#  for tup in tuples_sorted:\n",
    "#    if not current_group or abs(tup[1] - current_group[-1][1]) <= range_size:\n",
    "#        current_group.append(tup)\n",
    "#    else:\n",
    "#      groups.append(current_group)\n",
    "#      current_group = [tup]\n",
    "#  if current_group:\n",
    "#    groups.append(current_group)\n",
    "#  \n",
    "#  return groups\n",
    "\n",
    "# returns majority and outliers based on z-scores for the specified element in a list tuples\n",
    "def distinguish_outliers_by_front(tuples, z_threshold=1.0):\n",
    "  # Extract the second element from each tuple\n",
    "  second_terms = np.array([tup[1] for tup in tuples])\n",
    "  \n",
    "  # Calculate mean and standard deviation\n",
    "  mean = np.mean(second_terms)\n",
    "  std_dev = np.std(second_terms)\n",
    "  \n",
    "  if std_dev == 0:\n",
    "    # If all second terms are identical, classify all as \"majority\" and no \"outliers\"\n",
    "    return tuples, []\n",
    "  # Calculate z-scores for each second element\n",
    "  z_scores = (second_terms - mean) / std_dev\n",
    "  \n",
    "  # Classify tuples into \"majority\" and \"outliers\"\n",
    "  majority = [tuples[i] for i in range(len(tuples)) if abs(z_scores[i]) <= z_threshold]\n",
    "  outliers = [tuples[i] for i in range(len(tuples)) if abs(z_scores[i]) > z_threshold]\n",
    "  \n",
    "  return majority, outliers, mean\n",
    "\n",
    "# returns an array of RMS values from a signal over time specified by the window size \n",
    "def rms_over_windows(waveform_arr, window_size=100, silence_threshold = None):\n",
    "  rms_values = []\n",
    "  excluded_ranges = []\n",
    "  \n",
    "  start = 0\n",
    "  while start < len(waveform_arr):\n",
    "    end = start + window_size\n",
    "    if end > len(waveform_arr):\n",
    "      end = len(waveform_arr)\n",
    "    window = waveform_arr[start:end]\n",
    "    rms_value = calculate_rms(window)\n",
    "    rms_values.append(rms_value)\n",
    "    \n",
    "    if silence_threshold:\n",
    "      if rms_value < silence_threshold:\n",
    "        excluded_ranges.append((start, end))\n",
    "\n",
    "    start = end\n",
    "  \n",
    "  return rms_values, excluded_ranges\n",
    "\n",
    "def fft_max(waveform_arr : np.array):\n",
    "\n",
    "  # Pad with zeros if necessary \n",
    "  if(len(waveform_arr) != FFT_SIZE):\n",
    "    size_diff = abs(len(waveform_arr)-FFT_SIZE)\n",
    "    list(waveform_arr).extend([0] * size_diff)\n",
    "\n",
    "    # perform fft and get magnitude\n",
    "  fft = np.fft.fft(waveform_arr, FFT_SIZE)\n",
    "  fft_mag = np.abs(fft)\n",
    "  \n",
    "    #turn back to np.array\n",
    "  fft_mag = fft_mag[:FFT_SIZE // 2]\n",
    "  max_index = np.argmax(fft_mag)\n",
    "  return fft_mag, max_index\n",
    "\n",
    "def show_fft(fft_mag, start_sample:int, top:int=0):\n",
    "  from matplotlib.ticker import ScalarFormatter\n",
    "  \n",
    "  # make x-axis frequencies\n",
    "  frequencies = np.linspace(0, AUDIO_SIZE / 2, FFT_SIZE // 2)\n",
    "  out_path = f\".\\\\figures\\\\fft_{FILE_NAME.split('.')[0]}_{start_sample}.png\"\n",
    "  # Convert magnitude to dB\n",
    "  #fft_dB = 20 * np.log10((2/SEGMENT_LENGTH) * fft_mag)\n",
    "\n",
    "  # Plot the FFT magnitude as a bar plot\n",
    "  fig = plt.figure(figsize=FFT_FIG_SIZE)\n",
    "  ax = fig.add_subplot(111)\n",
    "  #plt.plot(frequencies, fft_dB)\n",
    "  plt.bar(frequencies, fft_mag, width=15, align='center', alpha=0.75)\n",
    "  plt.xscale('log')\n",
    "  plt.xticks([50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 3000, 4000, 5000, 8000 ],\n",
    "             ['50','100','200','300','400','500','600','700','800','900','1000','1500','2000','3000','4000','5000','8000']) \n",
    "  plt.xticks(rotation=45)  # Rotate by 45 degrees\n",
    "  plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "  plt.title(f\"FFT of \\\"{FILE_NAME}\\\" from {start_sample} to {start_sample+SEGMENT_LENGTH} samples\")\n",
    "  plt.xlabel(\"Frequency (Hz)\")\n",
    "  plt.ylabel(\"Magnitude\")\n",
    "  if(top):\n",
    "    largest = klargest_with_indices(fft_mag, top, (LOW_F0_INDEX, HIGH_F0_INDEX))\n",
    "    for index,value in largest:\n",
    "      max_x = AUDIO_SIZE/FFT_SIZE * index\n",
    "      max_y = value\n",
    "      ax.plot(max_x, max_y, 'ro')\n",
    "      ax.annotate(f'{max_x} Hz', xy=(max_x, max_y), xytext=(max_x+max_x/15, max_y),color='red')\n",
    "  plt.grid(True)\n",
    "  plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "  plt.close()\n",
    "  # Open the saved image with the default viewer\n",
    "  if OPEN_FFT:\n",
    "    if os.path.exists(out_path):\n",
    "      if os.name == 'nt':  # For Windows\n",
    "        os.startfile(out_path)\n",
    "\n",
    "  # Plot the waveform\n",
    "def show_waveform(waveform_arr, start_sample, zero_crossings:np.array=None, repcycle=None, filename=None):\n",
    "  \n",
    "  out_path = f\".\\\\figures\\\\{FILE_NAME.split('.')[0]}_{start_sample}.png\"\n",
    "  if filename:\n",
    "    out_path = f\".\\\\figures\\\\{filename}.png\"\n",
    "\n",
    "  fig = plt.figure(figsize=WAV_FIG_SIZE)\n",
    "  ax = fig.add_subplot(111)\n",
    "  plt.plot(range(start_sample, start_sample+len(waveform_arr)), waveform_arr)\n",
    "  y_max = max(abs(np.amin(waveform_arr)), abs(np.amax(waveform_arr)))\n",
    "\n",
    "  if y_max < 0.1: \n",
    "    y_max = 0.1\n",
    "    plt.ylim(-y_max, y_max)\n",
    "  if zero_crossings is not None and zero_crossings.size > 0 and SHOW_ZEROS:\n",
    "    plt.vlines(zero_crossings, -y_max, y_max, colors='red', linestyles=\"dashed\", linewidth=1)\n",
    "\n",
    "  if(repcycle):\n",
    "    #ax.fill_between(t, 1, where=s > 0, facecolor='green', alpha=.5)\n",
    "    #ax.fill_between(t, -1, where=s < 0, facecolor='red', alpha=.5)\n",
    "    ax.fill_between(repcycle, -y_max, y_max, facecolor='green', alpha=.25)\n",
    "  plt.xticks(np.arange(start_sample, start_sample+len(waveform_arr), 100))\n",
    "  plt.title(f\"Waveform of \\\"{FILE_NAME}\\\"\")\n",
    "  plt.xlabel(\"Samples\")\n",
    "  plt.ylabel(\"Amplitude\")\n",
    "  plt.grid()\n",
    "  plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "  plt.close()\n",
    "  # Open the saved image with the default viewer\n",
    "  if OPEN_WAV:\n",
    "    if os.path.exists(out_path):\n",
    "      if os.name == 'nt':  # For Windows\n",
    "        os.startfile(out_path)\n",
    "  \n",
    "def show_repcycles(repcycles, waveform_arr, vectorize=False):\n",
    "  # Find a pair of multiples of n_segments for dysplaing grid of waveforms \n",
    "  n_segments = AUDIO_SIZE / SEGMENT_LENGTH\n",
    "  min_diff = np.inf\n",
    "  row_col_pair = None\n",
    "  for pair in find_factor_pairs(n_segments):\n",
    "    diff = abs(pair[0]-pair[1])\n",
    "    if(diff < min_diff):\n",
    "      row_col_pair = pair\n",
    "      min_diff = diff\n",
    "  nrows, ncols = row_col_pair\n",
    "\n",
    "  ### make grid plot\n",
    "  _, axes = plt.subplots(nrows, ncols, figsize=(100, 50))\n",
    "\n",
    "  axes = axes.flatten()\n",
    "  # Loop through the data and corresponding axes to plot\n",
    "  for i, ax in enumerate(axes):\n",
    "    if i < len(repcycles):\n",
    "\n",
    "      if repcycles[i]:\n",
    "        \n",
    "        curr_repcycle = repcycles[i]\n",
    "        repc_start = math.floor(curr_repcycle[0])\n",
    "        repc_end = math.floor(curr_repcycle[1])\n",
    "        ticks = np.linspace(repc_start, repc_end, 10)\n",
    "        repcycle_wav = waveform_arr[repc_start:repc_end]\n",
    "        if vectorize:\n",
    "          ax.plot(np.linspace(repc_start,repc_end, REPC_VEC_SIZE), vectorize_f(repcycle_wav, REPC_VEC_SIZE))\n",
    "        else:\n",
    "          ax.plot(range(repc_start, repc_end), repcycle_wav)\n",
    "        ax.set_xticks(ticks)\n",
    "        y_max = max(abs(np.amin(repcycles[i])), abs(np.amax(repcycles[i])))   \n",
    "\n",
    "        if y_max < 0.1: \n",
    "          y_max = 0.1\n",
    "          plt.ylim(-y_max, y_max)\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "      else:\n",
    "        ax.text(0.5, 0.5, 'NONE', fontsize=50, ha='center', va='center', alpha=0.5)\n",
    "    else:\n",
    "      ax.axis('off') \n",
    "\n",
    "  out_path = f\".\\\\figures\\\\{FILE_NAME}_repc_segmentsize{SEGMENT_LENGTH}.png\"\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "  plt.close()\n",
    "  # Open the saved image with the default viewer\n",
    "  if OPEN_REPC:\n",
    "    if os.path.exists(out_path):\n",
    "      if os.name == 'nt':  # For Windows\n",
    "        os.startfile(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zerocrossings(waveform_arr:np.array, start_sample, dynamic_threshold=0.01):\n",
    "  last=None \n",
    "  zerocrossings = np.array([], dtype='f')\n",
    "  \n",
    "  #Filter out quiet parts of the signal using a RMS with windowing\n",
    "  _, excluded_ranges = rms_over_windows(waveform_arr, SEGMENT_LENGTH//5, dynamic_threshold)\n",
    "\n",
    "  # turn the excluded ranges into an array of samples that shouldn't be processed\n",
    "  excluded_samples = [] \n",
    "  for start, end in excluded_ranges:\n",
    "    excluded_samples.extend(range(start_sample + start, start_sample + end))\n",
    "\n",
    "  #sweep the whole segment\n",
    "  for index, value in enumerate(waveform_arr):\n",
    "    absolute_index = start_sample + index\n",
    "    if absolute_index in excluded_samples:\n",
    "      continue \n",
    "    if last == None:  #record last sample\n",
    "      last = value\n",
    "    #elif value == 0:\n",
    "    #  if np.sign(last) == -1.0 and np.sign(waveform_arr[index+1]) == 1.0:\n",
    "    #    np.append(zerocrossings, start_sample+index)\n",
    "    \n",
    "    # if last sample is negative and current one is positive a zero crossing occured\n",
    "    if np.sign(last) == -1.0 and np.sign(value) == 1.0: \n",
    "      # x = x1 + (x2-x1)/(y2-y1)*(y-y1)\n",
    "      inter_x = (index-1) + (-last/(value-last))\n",
    "      zerocrossings = np.append(zerocrossings,start_sample+inter_x)\n",
    "    last = value\n",
    "  return zerocrossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cycles_f0(f0, start_sample, zero_crossings:list):\n",
    "  \n",
    "  cycles = []\n",
    "  f0_length = 1.0/f0 * AUDIO_SIZE #length of samples of fundamental frequency\n",
    "  last_sample = start_sample*SEGMENT_LENGTH + SEGMENT_LENGTH \n",
    "  for start in zero_crossings:\n",
    "    cyclef0_end = start+f0_length # where we are expecting the cycle to end \n",
    "    \n",
    "    if cyclef0_end > last_sample+CYCLE_EPSILON: #don't look past end of segment, unless CYCLE_EPSILON allows you to see the end \n",
    "      continue\n",
    "\n",
    "    high = cyclef0_end + CYCLE_EPSILON #acceptabale ranges\n",
    "    low = cyclef0_end - CYCLE_EPSILON\n",
    "    \n",
    "    #get values that fall withing the f0+epsilon range\n",
    "    try:\n",
    "      possible_cycles = list(filter(lambda x, high=high, low=low: low <= x <= high ,zero_crossings))\n",
    "      if not possible_cycles:\n",
    "        #print(f\"Failed to find a cycle for sample {start} with an epsilon of {CYCLE_EPSILON}\")\n",
    "        continue\n",
    "      if len(possible_cycles) > 1:\n",
    "        # tie-breaker get the end sample that is closest to the expected\n",
    "        differences = list(map(lambda x, y=cyclef0_end: abs(y-x), possible_cycles))\n",
    "        end = possible_cycles[np.argmin(differences)]\n",
    "      else:\n",
    "        end = possible_cycles[0]\n",
    "    except Exception as e:\n",
    "      print(f\"Error: Failed to find a cycle for sample {start} with an epsilon of {CYCLE_EPSILON}\\n\",e)\n",
    "    cycles.append((start,end))\n",
    "  return cycles\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repcycle(segment_wav, start_sample, f0, cycles):\n",
    "  # 2. closeness to f0\n",
    "  # 3. loudness\n",
    "  # 4. most in the middle\n",
    "  \n",
    "  mid_segment = start_sample+ (SEGMENT_LENGTH/2);\n",
    "  f0_length = 1.0/f0 * AUDIO_SIZE #length of samples of fundamental frequency\n",
    "  \n",
    "  cycles_data = [] # (index, freq_score, rms_score, middle_score)\n",
    "  for index, cycle in enumerate(cycles):\n",
    "    \n",
    "    #get length of the cycle and comapre with fundamental frequency reciprocal \n",
    "    freq = cycle[1] - cycle[0]\n",
    "    f_score = abs(freq - f0_length)\n",
    "    \n",
    "    #get middle and comare with middle of the whole segment  \n",
    "    middle = (cycle[0] + cycle[1]) / 2\n",
    "    m_score = abs(middle - mid_segment)\n",
    "\n",
    "    #get rms\n",
    "    i_beginning = math.floor(cycle[0]-start_sample)\n",
    "    i_end = math.floor(cycle[1]-start_sample)\n",
    "    cyc_wav = segment_wav[i_beginning:i_end]\n",
    "    rms = calculate_rms(cyc_wav)\n",
    "    r_score = 1-rms\n",
    "\n",
    "    #identify cycles that have an obvious shape at the beginning of the cycle by using bias_front\n",
    "\n",
    "    #cyc_wavs.append(cyc_wav)\n",
    "    cycles_data.append((index, f_score, r_score, m_score))\n",
    "    \n",
    "  # Normalize scores\n",
    "  max_f_score = max(cycles_data, key=lambda x: x[1])[1]\n",
    "  max_r_score = max(cycles_data, key=lambda x: x[2])[2]\n",
    "  max_m_score = max(cycles_data, key=lambda x: x[3])[3]\n",
    "\n",
    "  normalized_cycles_data = [\n",
    "    (index, f_score / max_f_score, r_score / max_r_score, m_score / max_m_score)\n",
    "    for index, f_score, r_score, m_score in cycles_data\n",
    "  ]\n",
    "\n",
    "  # Apply weights and sort\n",
    "  weighted_cycles_data = [\n",
    "    (index, f_score * FREQ_WEIGHT, r_score * RMS_WEIGHT, m_score * MID_WEIGHT)\n",
    "    for index, f_score, r_score, m_score in normalized_cycles_data\n",
    "  ]\n",
    "\n",
    "    #loop throuh all combination of cycles to take the dot product of all, forward only\n",
    "  #for cyc1_index, cyc1 in enumerate(cyc_wavs):\n",
    "  #  if cyc1_index > len(cyc_wavs)-1:\n",
    "  #    break\n",
    "  #  for j, cyc2 in enumerate(cyc_wavs[cyc1_index+1:]):\n",
    "  #    cyc2_index = j+cyc1_index+1\n",
    "  #    \n",
    "  #    #pad with zeroes if cycles are not same size\n",
    "  #    if len(cyc1) < len(cyc2):\n",
    "  #      cyc1 = np.pad(cyc1,(0,len(cyc2)-len(cyc1)))\n",
    "  #    elif len(cyc2) < len(cyc1):\n",
    "  #      cyc2 = np.pad(cyc2,(0,len(cyc1)-len(cyc2)))\n",
    "  #    cyc_dots.append(np.dot(cyc1,cyc2), cyc1_index, cyc2_index) # (dot_prod, cyc1_index , cyc2_index) \n",
    "  \n",
    "  \n",
    "  \n",
    "  sorted_list = sorted(weighted_cycles_data, key=lambda x: x[1]+x[2]+x[3])\n",
    "  #assert sorted_list != [], f\"No repcycles found at {start_sample}\"\n",
    "  if sorted_list == []:\n",
    "    return None\n",
    "  return cycles[sorted_list[0][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repcycle2(segment_wav, start_sample, f0, cycles):\n",
    "  #score the cycles based on they adherence to the following biases:\n",
    "  # TODO make algorithm to ignore noisy sections of the signal\n",
    "  # 1. obvious shape at the beginning of the cycle\n",
    "  # 2. closeness to f0\n",
    "  # 3. loudness\n",
    "  # 4. most in the middle\n",
    "  \n",
    "  if len(cycles) < 2:\n",
    "    return cycles[0]  #if there is only one cycle return it\n",
    "  \n",
    "  mid_segment = start_sample+ (SEGMENT_LENGTH/2);\n",
    "  f0_length = 1.0/f0 * AUDIO_SIZE #length of samples of fundamental frequency\n",
    "  \n",
    "  cycles_data = [] # (index, front, freq_score, rms_score, middle_score)\n",
    "  for index, cycle in enumerate(cycles):\n",
    "    \n",
    "    i_beginning = math.floor(cycle[0]-start_sample)\n",
    "    i_end = math.floor(cycle[1]-start_sample)\n",
    "    cyc_wav = segment_wav[i_beginning:i_end]\n",
    "\n",
    "    cyc_vec = vectorize_f(cyc_wav, FRONT_VEC_SIZE)\n",
    "    front = bias_front_abs(cyc_vec)\n",
    "\n",
    "    #get length of the cycle and comapre with fundamental frequency reciprocal \n",
    "    freq = cycle[1] - cycle[0]\n",
    "    f_score = abs(freq - f0_length)\n",
    "    \n",
    "    #get middle and comare with middle of the whole segment  \n",
    "    middle = (cycle[0] + cycle[1]) / 2\n",
    "    m_score = abs(middle - mid_segment)\n",
    "\n",
    "    #get rms\n",
    "    rms = calculate_rms(cyc_wav)\n",
    "    r_score = 1-rms\n",
    "\n",
    "    #identify cycles that have an obvious shape at the beginning of the cycle by using bias_front\n",
    "\n",
    "    #cyc_wavs.append(cyc_wav)\n",
    "    cycles_data.append((index, front, f_score, r_score, m_score))\n",
    "\n",
    "    \n",
    "    # There seems to ba a positive correlation between the number of zero crossings and the fundamental frequency \n",
    "    # As the fundamental frequency increases because of either a higher pitch or more overtones in the upper register\n",
    "    # There will be more zerocrossings. This can be intuitively observed as raising the pitch of a sine wave is equivalent\n",
    "    # to squishing the waveform in-place and therefore increasing the number of zero crossings.\n",
    "    # we can apply different cases of filtering for the most representative cycle by looking at the number of zero crossings\n",
    "    \n",
    "    # - For little zero crossing there is no need to find a majority, as we dont want to restrict our search\n",
    "    # - A search for a majority comes in handy for medium number of zero crossings which can be indicitive of a steady pitch\n",
    "    # - For a signal with many zero crossing (many overtones), a majority can be misleading and instead we should look for oultiers \n",
    "    # as they will have a larger front_bias\n",
    "\n",
    "  if(len(cycles_data) <= 4): #No need to distinguish outliers if there are only 4 or less cycles\n",
    "    filtered_list = cycles_data\n",
    "\n",
    "  elif(len(cycles_data) > SEGMENT_LENGTH//50): # outliers may be more representative of the signal\n",
    "    majority, outliers, mean = distinguish_outliers_by_front(cycles_data, Z_THRESHOLD)\n",
    "    higher_outliers = [tup for tup in outliers if tup[1] > mean] \n",
    "    if higher_outliers:\n",
    "      filtered_list = higher_outliers  \n",
    "    else:\n",
    "      filtered_list = majority\n",
    "  else: \n",
    "    majority, outliers, mean = distinguish_outliers_by_front(cycles_data, Z_THRESHOLD)\n",
    "    filtered_list = majority\n",
    "  \n",
    "\n",
    "    # Normalize scores\n",
    "  max_f_score = max(filtered_list, key=lambda x: x[2])[2]\n",
    "  max_r_score = max(filtered_list, key=lambda x: x[3])[3]\n",
    "  max_m_score = max(filtered_list, key=lambda x: x[4])[4]\n",
    "\n",
    "  normalized_cycles_data = [\n",
    "    (index, front, f_score / max_f_score, r_score / max_r_score, m_score / max_m_score)\n",
    "    for index, front, f_score, r_score, m_score in filtered_list\n",
    "  ]\n",
    "\n",
    "  # Apply weights and sort\n",
    "  weighted_cycles_data = [\n",
    "    (index, front, f_score / FREQ_WEIGHT, r_score / RMS_WEIGHT, m_score / MID_WEIGHT)\n",
    "    for index, front, f_score, r_score, m_score in normalized_cycles_data\n",
    "  ]\n",
    "  \n",
    "  sorted_list = sorted(weighted_cycles_data, key=lambda x: x[2]+x[3]+x[4])\n",
    "  assert sorted_list != [], f\"No repcycles found at {start_sample}\"\n",
    "  return cycles[sorted_list[0][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repcycle3(segment_wav, start_sample, f0, cycles):\n",
    "  #score the cycles based an obvious shape at the beginning of the cycle\n",
    "  \n",
    "  if len(cycles) < 2:\n",
    "    return cycles[0]  #if there is only one cycle return it\n",
    "  \n",
    "  mid_segment = start_sample+ (SEGMENT_LENGTH/2);\n",
    "  f0_length = 1.0/f0 * AUDIO_SIZE #length of samples of fundamental frequency\n",
    "\n",
    "  cycles_data = [] # (index, front, front_abs, f_score, m_score)\n",
    "  for index, cycle in enumerate(cycles):\n",
    "    \n",
    "    i_beginning = math.floor(cycle[0]-start_sample)\n",
    "    i_end = math.floor(cycle[1]-start_sample)\n",
    "    cyc_wav = segment_wav[i_beginning:i_end]\n",
    "\n",
    "    #identify cycles that have an obvious shape at the beginning of the cycle \n",
    "    cyc_vec = vectorize_f(cyc_wav, FRONT_VEC_SIZE)\n",
    "    front = bias_front_abs(cyc_vec)\n",
    "\n",
    "    #get length of the cycle and comapre with fundamental frequency reciprocal \n",
    "    freq = cycle[1] - cycle[0]\n",
    "    f_score = abs(freq - f0_length)\n",
    "\n",
    "    #get middle and comare with middle of the whole segment  \n",
    "    middle = (cycle[0] + cycle[1]) / 2\n",
    "    m_score = abs(middle - mid_segment)\n",
    "\n",
    "    cycles_data.append((index, front, f_score, m_score))\n",
    "\n",
    "  if(len(cycles_data) <= 4): #No need to distinguish outliers if there are only 4 or less cycles\n",
    "    candidates_list = cycles_data\n",
    "  else:\n",
    "    front_mean =  np.mean([tup[1] for tup in cycles_data])\n",
    "    filtered_list = list(filter(lambda x, front_mean_abs=front_mean: x[1] > front_mean_abs, cycles_data))\n",
    "\n",
    "    if len(filtered_list) > 1:\n",
    "      candidates_list = filtered_list\n",
    "    else:\n",
    "      candidates_list = cycles_data\n",
    "\n",
    "  assert candidates_list != [], f\"No repcycles found at {start_sample} to {start_sample+SEGMENT_LENGTH}\"\n",
    "\n",
    "    # Normalize scores\n",
    "  max_f_score = max(candidates_list, key=lambda x: x[2])[2]\n",
    "  max_m_score = max(candidates_list, key=lambda x: x[3])[3]\n",
    "\n",
    "  normalized_cycles_data = [\n",
    "    (index, front, f_score / max_f_score, m_score / max_m_score)\n",
    "    for index, front,  f_score, m_score in candidates_list\n",
    "  ]\n",
    "\n",
    "  # Apply weights and sort\n",
    "  weighted_cycles_data = [\n",
    "    (index, front,  f_score / FREQ_WEIGHT, m_score / MID_WEIGHT)\n",
    "    for index, front,  f_score, m_score in normalized_cycles_data\n",
    "  ]\n",
    "  \n",
    "  sorted_list = sorted(weighted_cycles_data, key=lambda x: x[2]+x[3])\n",
    "  assert sorted_list != [], f\"No repcycles found at {start_sample} to {start_sample+SEGMENT_LENGTH}\"\n",
    "  return cycles[sorted_list[0][0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#waveform, sample_rate = torchaudio.load(AUDIO_PATH + FILE_NAME)\n",
    "#waveform_arr = waveform.t().squeeze(1).numpy()\n",
    "#\n",
    "#split_waveform = np.array_split(waveform_arr, np.arange(SEGMENT_LENGTH, AUDIO_SIZE, SEGMENT_LENGTH))\n",
    "#semgent_wav = split_waveform[SEGMENT_NUM]\n",
    "##wav_vector = vectorize_f(semgent_wav,80)\n",
    "#\n",
    "##wav_vector = bias_front(split_waveform[SEGMENT_NUM],40)\n",
    "##1show_waveform(semgent_wav, SEGMENT_NUM*SEGMENT_LENGTH)\n",
    "##1show_waveform(wav_vector, SEGMENT_NUM*SEGMENT_LENGTH, filename=f\"vectorized{SEGMENT_NUM*SEGMENT_LENGTH}\")\n",
    "#\n",
    "#front = bias_front(semgent_wav)\n",
    "#print(front)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Segment Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No zero crossings were found for segment [9500, 10000]\n",
      "Error No cycles were found for segment [9500, 10000]\n"
     ]
    }
   ],
   "source": [
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH + FILE_NAME)\n",
    "waveform_arr = waveform.t().squeeze(1).numpy()\n",
    "\n",
    "# do not try to comprehend this....... I myself stopped trying to. This is an attempt to make the the algoithm automatically filter out noise \n",
    "# Signal-to-Noise Ratio or rather it's reciprocal Noise-to-Signal Ratio (NSR) is unique to a audio file and can be used to figure out how noisy it is\n",
    "# NSR by itself is too low of a value to use as a filter hence the magic number NSR_FACTOR\n",
    "# also it doesn't work............ hence the deprecation\n",
    "\n",
    "#identify SNR reciprocal to get a dynamic noise thresholds for filtering \n",
    "rms_values, _ = rms_over_windows(waveform_arr, SEGMENT_LENGTH) #calculate RMS for each segment\n",
    "\n",
    "# Get Noise-to-Signal Ratio\n",
    "#signal_rms = np.max(rms_values)\n",
    "#noise_rms = np.mean(rms_values)\n",
    "#nsr = noise_rms/ signal_rms\n",
    "\n",
    "# Calculate dynamic silence threshold based on NSR\n",
    "#silence_1threshold = min(nsr * NSR_FACTOR, signal_rms*.50)\n",
    "\n",
    "# Get Noise-to-Signal Ratio\n",
    "signal_rms = np.max(rms_values)\n",
    "noise_rms = np.mean(rms_values) * NOISE_FACTOR\n",
    "\n",
    "# Calculate dynamic silence threshold based on NSR\n",
    "silence_threshold = min(noise_rms, signal_rms*.50)\n",
    "\n",
    "# Split the waveform into segments\n",
    "split_waveform = np.array_split(waveform_arr, np.arange(SEGMENT_LENGTH, AUDIO_SIZE, SEGMENT_LENGTH))\n",
    "#split_waveform = [waveform_np[i:i+segment_length] for i in range(0, 16000, segment_length)]\n",
    "\n",
    "####### TESTING - get one segment ############\n",
    "segment_wav = split_waveform[SEGMENT_NUM]\n",
    "start_sample = SEGMENT_NUM*SEGMENT_LENGTH\n",
    "\n",
    "# Get the Zero-Crossings, ignoring noise\n",
    "zero_crossings = find_zerocrossings(segment_wav, start_sample, silence_threshold)\n",
    "if len(zero_crossings) < 2:\n",
    "  print(f\"No zero crossings were found for segment [{start_sample}, {start_sample+SEGMENT_LENGTH}]\")\n",
    "\n",
    "# Get the fundamental frequency using the FFT\n",
    "fft_mag, max_index = fft_max(segment_wav)\n",
    "\n",
    "# exclude bins outside the F0_FREQ_RANGE\n",
    "top_bins = klargest_with_indices(fft_mag, 4, (LOW_F0_INDEX,HIGH_F0_INDEX))\n",
    "#itop_in_range = list(filter(lambda x, h=LOW_F0, l = HIGH_F0: l <= x[0] <= h, top_bins))[0]\n",
    "\n",
    "# get frequency to bin index\n",
    "# Fundamental Frequency: Sample Rate / fftsize * fft_bin_index\n",
    "f0 = AUDIO_SIZE/FFT_SIZE * top_bins[0][0]\n",
    "\n",
    "if len(top_bins) == 0:\n",
    "  print(f\"Failed to find a fundamental frequency in range ({F0_FREQ_RANGE[0]}Hz ,{F0_FREQ_RANGE[1]}Hz)\")\n",
    "#else:\n",
    "  #print(f\"f0: {f0}\")\n",
    "#print(f\"silence_threshold: {silence_threshold}\")\n",
    "#print(f\"F0 magnitude {fft_mag[max_index]}\")\n",
    "\n",
    "#find cycles within the signal and find a representative one for the segment\n",
    "\n",
    "cycles = find_cycles_f0(f0, start_sample, zero_crossings)\n",
    "if len(cycles) == 0: \n",
    "  print(f\"Error No cycles were found for segment [{start_sample}, {start_sample+SEGMENT_LENGTH}]\")\n",
    "\n",
    "repcycle = None\n",
    "if len(cycles) > 0:\n",
    "  repcycle = find_repcycle3(segment_wav, start_sample, f0, cycles)\n",
    "  if repcycle == None:\n",
    "    print(f\"Error: failed to find a representative cycle for segment [{start_sample}, {start_sample+SEGMENT_LENGTH}]\")\n",
    "\n",
    "show_waveform(segment_wav, start_sample, zero_crossings, repcycle)\n",
    "show_fft(fft_mag, start_sample, 4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:107: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:107: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:107: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:107: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\Andrei\\AppData\\Local\\Temp\\ipykernel_2116\\747639225.py:107: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  torchaudio.save(f\"out\\{FILE_NAME.split(\".\")[0]}_size{SEGMENT_LENGTH}out.wav\", outfile_tensor, 16000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH + FILE_NAME)\n",
    "waveform_arr = waveform.t().squeeze(1).numpy()\n",
    "\n",
    "#identify SNR reciprocal to get a dynamic noise threshold for filtering \n",
    "rms_values, _ = rms_over_windows(waveform_arr, SEGMENT_LENGTH) #calculate RMS over time\n",
    "\n",
    "# Get Noise-to-Signal Ratio\n",
    "#signal_rms = np.max(rms_values)\n",
    "#noise_rms = np.mean(rms_values)\n",
    "#nsr = noise_rms/ signal_rms\n",
    "\n",
    "# Calculate dynamic silence threshold based on NSR\n",
    "#silence_1threshold = min(nsr * NSR_FACTOR, signal_rms*.50)\n",
    "\n",
    "# Get Noise-to-Signal Ratio\n",
    "signal_rms = np.max(rms_values)\n",
    "noise_rms = np.mean(rms_values) * NOISE_FACTOR\n",
    "\n",
    "# Calculate dynamic silence threshold based on NSR\n",
    "silence_threshold = min(noise_rms, signal_rms*.50)\n",
    "\n",
    "# Split the waveform into segments. fortunately this also pads the file if it was too short to begin with - serendipity \n",
    "split_waveform = np.array_split(waveform_arr, np.arange(SEGMENT_LENGTH, AUDIO_SIZE, SEGMENT_LENGTH))\n",
    "#split_waveform = [waveform_np[i:i+segment_length] for i in range(0, 16000, segment_length)]\n",
    "\n",
    "repcycles = []\n",
    "\n",
    "for segment_num, segment_wav in enumerate(split_waveform):\n",
    "  #segment_wav = split_waveform[SEGMENT_NUM]s\n",
    "  start_sample = segment_num*SEGMENT_LENGTH\n",
    "  # Get the Zero-Crossings, ignoring noise\n",
    "  zero_crossings = find_zerocrossings(segment_wav, start_sample, silence_threshold)\n",
    "  if not len(zero_crossings) > 1:\n",
    "    repcycles.append([])\n",
    "  # Get the fundamental frequency using the FFT\n",
    "    continue\n",
    "  fft_mag, max_index = fft_max(segment_wav)\n",
    "\n",
    "  # exclude bins outside the F0_FREQ_RANGE\n",
    "  # get frequency to bin index\n",
    "\n",
    "  top_bins = klargest_with_indices(fft_mag, 4, (LOW_F0_INDEX,HIGH_F0_INDEX))\n",
    "  assert len(top_bins) > 0, f\"Failed to find a fundamental frequency in range ({F0_FREQ_RANGE[0]}Hz ,{F0_FREQ_RANGE[1]}Hz)\"\n",
    "  #top_in_range = list(filter(lambda x, h=LOW_F0, l = HIGH_F0: l <= x[0] <= h, top_bins))[0]\n",
    "\n",
    "  # Fundamental Frequency: Sample Rate / fftsize * fft_bin_index\n",
    "  f0 = AUDIO_SIZE/FFT_SIZE * top_bins[0][0]\n",
    "\n",
    "  #print(f\"silence_threshold: {silence_threshold}\")\n",
    "  #print(f\"f0: {f0}\")\n",
    "  #print(f\"F0 magnitude{fft_mag[max_index]}\")\n",
    "\n",
    "  #find cycles within the signal and find a representative one for the segment\n",
    "  cycles = find_cycles_f0(f0, start_sample, zero_crossings)\n",
    "  if not len(cycles) > 0:\n",
    "    repcycles.append([])\n",
    "    continue\n",
    "\n",
    "  repcycle = find_repcycle3(segment_wav, start_sample, f0, cycles)\n",
    "  assert repcycle != None, f\"Error: failed to find a representative cycle for segment [{start_sample}, {start_sample+SEGMENT_LENGTH}]\"\n",
    "\n",
    "  repcycles.append(repcycle)\n",
    "  #show_waveform(segment_wav, start_sample, zero_crossings, repcycle)\n",
    "  #show_fft(fft_mag, start_sample, 4)\n",
    "\n",
    "show_repcycles(repcycles, waveform_arr, vectorize=True)\n",
    "\n",
    "#for num in cycles:\n",
    "#  print(f\"({num[0]:.2f}, {num[1]:.2f})\")\n",
    "#with open('fft_freq.txt', 'w') as file:\n",
    "#  for index, value in enumerate(fft_mag):\n",
    "#    file.write(f\"{value} at {index*AUDIO_SIZE/FFT_SIZE}\")\n",
    "\n",
    "#### TEST- output repcycle to text ####\n",
    "#with open(f'out\\{FILE_NAME.split('.')[0]}_repcycles.txt', 'w') as file:\n",
    "#  for index, value in enumerate(repcycles):\n",
    "#    if value:\n",
    "#      file.write(\"%.3f, %.3f\\n\" % (value[0], value[1]))\n",
    "#    else:\n",
    "#      file.write(\"0\\n\")\n",
    "#################################################################\n",
    "\n",
    "#### TEST- output repcycle AUDIO ####\n",
    "#for index, curr_repcycle in enumerate(repcycles):\n",
    "#  if curr_repcycle:\n",
    "#    repc_start = math.floor(curr_repcycle[0])\n",
    "#    repc_end = math.floor(curr_repcycle[1])\n",
    "#    rep_cycle_wav = waveform_arr[repc_start:repc_end]    \n",
    "#    outfile_tensor = torch.tensor(rep_cycle_wav).unsqueeze(0)\n",
    "#    torchaudio.save(f\"out\\{FILE_NAME.split(\".\")[0]}_{index*SEGMENT_LENGTH}.wav\", outfile_tensor, 16000)\n",
    "#################################################################\n",
    "\n",
    "#### TEST- output audio 10 copies of repcycle ####\n",
    "outfile_wav = []\n",
    "for curr_repcycle in repcycles:\n",
    "  if curr_repcycle:\n",
    "    repc_start = math.floor(curr_repcycle[0])\n",
    "    repc_end = math.floor(curr_repcycle[1])\n",
    "    repcycle_wav = waveform_arr[repc_start:repc_end]\n",
    "    #repcycle_wav_v = vectorize_f(repcycle_wav, REPC_VEC_SIZE)\n",
    "    for _ in range(10):\n",
    "      outfile_wav.extend(repcycle_wav)\n",
    "  #else:\n",
    "  #  outfile_wav.extend(np.zeros(REPC_VEC_SIZE))  # pad with silence if no repcycle found\n",
    "    \n",
    "outfile_tensor = torch.tensor(outfile_wav).unsqueeze(0)\n",
    "torchaudio.save(f\"out\\{FILE_NAME.split(\".\")[0]}_size{SEGMENT_LENGTH}out.wav\", outfile_tensor, 16000)\n",
    "################################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
