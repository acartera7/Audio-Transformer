{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# Visual Transformer based off the original publication An Image is Worth 16x16 Words:\n",
    "# https://arxiv.org/pdf/2010.11929\n",
    "\n",
    "# Code is taken from a tutorial by Brian Pulfer\n",
    "# https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "# Andrei Cartera -- Oct 2024\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToTensor, Compose, Resize\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: console dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert notebook run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "NUM_CLASSES   = 10\n",
    "N_EPOCHS      = 5\n",
    "LR            = 0.005\n",
    "\n",
    "CHW           = (1,28,28)\n",
    "N_PATCHES     = 7\n",
    "N_BLOCKS      = 2\n",
    "HIDDEN_DIM    = 8\n",
    "N_HEADS       = 2\n",
    "PATCH_SIZE    = 4\n",
    "DROPOUT       = 0.001\n",
    "ACTIVATION    =\"gelu\"\n",
    "NUM_ENCODERS  = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "  n, c, h, w = images.shape\n",
    "\n",
    "  assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "  patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "  patch_size = h // n_patches\n",
    "\n",
    "  for idx, image in enumerate(images):\n",
    "    for i in range(n_patches):\n",
    "      for j in range(n_patches):\n",
    "          patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "          patches[idx, i * n_patches + j] = patch.flatten()\n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_patchify(images, n_patches):\n",
    "  n, c, h, w = images.shape\n",
    "  assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "  patch_size = h // n_patches\n",
    "  patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "  patches = patches.contiguous().view(n, c, -1, patch_size, patch_size)\n",
    "  patches = patches.permute(0, 2, 1, 3, 4).contiguous().view(n, -1, patch_size * patch_size * c)\n",
    "  \n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "  result = torch.ones(sequence_length, d)\n",
    "  for i in range(sequence_length):\n",
    "      for j in range(d):\n",
    "          result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads=2):\n",
    "    super(MyMSA, self).__init__()\n",
    "    self.d = d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "    d_head = int(d / n_heads)\n",
    "    self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.d_head = d_head\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    # Sequences has shape (N, seq_length, token_dim)\n",
    "    # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "    # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "        seq_result = []\n",
    "        for head in range(self.n_heads):\n",
    "            q_mapping = self.q_mappings[head]\n",
    "            k_mapping = self.k_mappings[head]\n",
    "            v_mapping = self.v_mappings[head]\n",
    "\n",
    "            seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "            q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "            attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "            seq_result.append(attention @ v)\n",
    "        result.append(torch.hstack(seq_result))\n",
    "    return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implamentation of Encoder Block\n",
    "class MyViTBlock(nn.Module):\n",
    "  def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "    super(MyViTBlock, self).__init__()\n",
    "    self.hidden_d = hidden_d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(hidden_d)\n",
    "    self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "    self.norm2 = nn.LayerNorm(hidden_d)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "        nn.GELU(),\n",
    "        nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mhsa(self.norm1(x))\n",
    "    out = out + self.mlp(self.norm2(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "  def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "    # Super constructor\n",
    "    super(MyViT, self).__init__()\n",
    "    \n",
    "    # Attributes\n",
    "    self.chw = chw # ( C , H , W )\n",
    "    self.n_patches = n_patches\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_heads = n_heads\n",
    "    self.hidden_d = hidden_d\n",
    "    \n",
    "    # Input and patches sizes\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "    # 1) Linear mapper\n",
    "    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "    \n",
    "    # 2) Learnable classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "    \n",
    "    # 3) Positional embedding\n",
    "    self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "    \n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "    \n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self.hidden_d, out_d),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    patches = optimized_patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    tokens = self.linear_mapper(patches)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "        \n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    \n",
    "    return self.mlp(out) # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10])\n"
     ]
    }
   ],
   "source": [
    "# Current model\n",
    "model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10).to(device)\n",
    "\n",
    "imgs = torch.randn(7, 1, 28, 28).to(device) # Dummy images\n",
    "print(model(imgs).shape) # torch.Size([7, 49, 16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Loading data\n",
    "  transform = Compose([Resize(28), ToTensor()])\n",
    "\n",
    "  filepath = Path('.')\n",
    "\n",
    "  train_set = MNIST(root=filepath.joinpath('../datasets'), train=True, download=True, transform=transform)\n",
    "  test_set = MNIST(root=filepath.joinpath('../datasets'), train=False, download=True, transform=transform)\n",
    "\n",
    "  train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "  test_loader = DataLoader(test_set, shuffle=False, batch_size=128)\n",
    "\n",
    "  # Defining model and training options\n",
    "  print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "  model = MyViT(CHW, N_PATCHES, N_BLOCKS, HIDDEN_DIM, N_HEADS, out_d=10).to(device)\n",
    "\n",
    "  # Training loop\n",
    "  optimizer = Adam(model.parameters(), lr=LR)\n",
    "  criterion = CrossEntropyLoss()\n",
    "  scheduler = lr_scheduler.LinearLR(optimizer)\n",
    "  \n",
    "  for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "      x, y = batch\n",
    "      print(f\"x: {type(x)}\\ny: {type(y)}\")\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "\n",
    "      train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "    \n",
    "    scheduler.step()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "  torch.save(model.state_dict(), 'my_model3.pth')\n",
    "\n",
    "  # Test loop\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "      correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "      total += len(x)\n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cpu \n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \n\u001b[1;32m----> 2\u001b[0m   \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 22\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m criterion \u001b[38;5;241m=\u001b[39m CrossEntropyLoss()\n\u001b[0;32m     20\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m lr_scheduler\u001b[38;5;241m.\u001b[39mLinearLR(optimizer)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     23\u001b[0m   train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in training\u001b[39m\u001b[38;5;124m\"\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\andre\\Documents\\SVN\\MUS470-470L\\Project\\Source\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:312\u001b[0m, in \u001b[0;36mtnrange\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtnrange\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    311\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Shortcut for `tqdm.notebook.tqdm(range(*args), **kwargs)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 312\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtqdm_notebook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\andre\\Documents\\SVN\\MUS470-470L\\Project\\Source\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\andre\\Documents\\SVN\\MUS470-470L\\Project\\Source\\.venv\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "  main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
