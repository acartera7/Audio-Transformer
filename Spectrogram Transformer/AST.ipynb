{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# Visual Transformer based off the original publication An Image is Worth 16x16 Words:\n",
    "# https://arxiv.org/pdf/2010.11929\n",
    "\n",
    "# Code is taken from a tutorial by Brian Pulfer\n",
    "# https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "# Andrei Cartera -- Oct 2024\n",
    "\n",
    "import datetime\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import CustomSpeechCommands as SpeechCommands\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrei\\Documents\\GitHub\\Audio-Transformer\\Spectrogram Transformer\\.venv\\Scripts\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "CHW = (1,224,224)\n",
    "NUM_CLASSES = 10\n",
    "N_PATCHES = 28\n",
    "N_HEADS = 8\n",
    "N_ENCODERS = 4\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 16\n",
    "\n",
    "DROPOUT = 0.05\n",
    "ACTIVATION=\"gelu\"\n",
    "LR = 0.001\n",
    "\n",
    "\n",
    "'ASTmodel_E10_200_25_4_4_25_32.pth'\n",
    "today = datetime.date.today()\n",
    "\n",
    "MODEL_PATH = f'models/({today})_ASTmodel_E{EPOCHS}_S{CHW[1]}_{N_HEADS}_{N_ENCODERS}_B{BATCH_SIZE}_H{HIDDEN_DIM}.pth'\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "  n, c, h, w = images.shape\n",
    "\n",
    "  assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "  patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "  patch_size = h // n_patches\n",
    "\n",
    "  for idx, image in enumerate(images):\n",
    "    for i in range(n_patches):\n",
    "      for j in range(n_patches):\n",
    "        patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "        patches[idx, i * n_patches + j] = patch.flatten()\n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_patchify(images, n_patches):\n",
    "  n, c, h, w = images.shape\n",
    "  assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "  patch_size = h // n_patches\n",
    "  patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "  patches = patches.contiguous().view(n, c, -1, patch_size, patch_size)\n",
    "  patches = patches.permute(0, 2, 1, 3, 4).contiguous().view(n, -1, patch_size * patch_size * c)\n",
    "  \n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "  result = torch.ones(sequence_length, d)\n",
    "  for i in range(sequence_length):\n",
    "    for j in range(d):\n",
    "      result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 625, 64])\n"
     ]
    }
   ],
   "source": [
    "image = torch.rand(1,1,200,200)\n",
    "patches = patchify(image, 25)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads=2):\n",
    "    super(MyMSA, self).__init__()\n",
    "    self.d = d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "    d_head = int(d / n_heads)\n",
    "    self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.d_head = d_head\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    # Sequences has shape (N, seq_length, token_dim)\n",
    "    # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "    # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "      seq_result = []\n",
    "      for head in range(self.n_heads):\n",
    "        q_mapping = self.q_mappings[head]\n",
    "        k_mapping = self.k_mappings[head]\n",
    "        v_mapping = self.v_mappings[head]\n",
    "\n",
    "        seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "        q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "        attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "        seq_result.append(attention @ v)\n",
    "      result.append(torch.hstack(seq_result))\n",
    "    return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "  def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "    super(MyViTBlock, self).__init__()\n",
    "    self.hidden_d = hidden_d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(hidden_d)\n",
    "    self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "    self.norm2 = nn.LayerNorm(hidden_d)\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mhsa(self.norm1(x))\n",
    "    out = out + self.mlp(self.norm2(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "  def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "    # Super constructor\n",
    "    super(MyViT, self).__init__()\n",
    "    \n",
    "    # Attributes\n",
    "    self.chw = chw # ( C , H , W )\n",
    "    self.n_patches = n_patches\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_heads = n_heads\n",
    "    self.hidden_d = hidden_d\n",
    "    \n",
    "    # Input and patches sizes\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "    # 1) Linear mapper\n",
    "    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "    \n",
    "    # 2) Learnable classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "    \n",
    "    # 3) Positional embedding\n",
    "    self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "    \n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "    \n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self.hidden_d, out_d),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    patches = optimized_patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    tokens = self.linear_mapper(patches)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "        \n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    \n",
    "    return self.mlp(out) # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Loading data\n",
    "\n",
    "  print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "  model = MyViT(CHW, N_PATCHES, N_ENCODERS, HIDDEN_DIM, N_HEADS, NUM_CLASSES).to(device)\n",
    "  #model.load_state_dict(torch.load('my_model3_2_Transfer.pth',map_location=torch.device('cpu')))\n",
    "  #https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "  train_set = SpeechCommands.CustomSpeechCommandsDataset(\"../custom_speech_commands\", shuffle=True, divisor=BATCH_SIZE, out_size=(224,224))\n",
    "  test_set = SpeechCommands.CustomSpeechCommandsDataset(\"../custom_speech_commands\", subset=\"testing\", shuffle=True,divisor=BATCH_SIZE, out_size=(224,224))\n",
    "  \n",
    "  train_loader = DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "  test_loader = DataLoader(test_set, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "  # Defining model and training options\n",
    "\n",
    "  # Training loop\n",
    "  optimizer = Adam(model.parameters(), lr=LR)\n",
    "  criterion = CrossEntropyLoss()\n",
    "  scheduler = lr_scheduler.LinearLR(optimizer)\n",
    "  #scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "                                             \n",
    "  for epoch in trange(EPOCHS, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "      x, y = batch\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item() * x.size(0)\n",
    "      \n",
    "    train_loss /= len(train_loader.dataset) \n",
    "    scheduler.step(train_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "  torch.save(model.state_dict(), 'ASTmodel_E30_224_28_8_4_32_16.pth')\n",
    "\n",
    "  # Test loop\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "      correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "      total += len(x)\n",
    "      \n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:0 (NVIDIA GeForce RTX 4060)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrei\\Documents\\GitHub\\Audio-Transformer\\Spectrogram Transformer\\.venv\\Lib\\site-packages\\torchaudio\\transforms\\_transforms.py:580: UserWarning: Argument 'onesided' has been deprecated and has no influence on the behavior of this module.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e544e1bd7dae4dc3b13d664438b91cad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e9ca8f669b64e1990282179fb39354e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 in training:   0%|          | 0/1030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:  \n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m   \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m loss = criterion(y_hat, y)\n\u001b[32m     32\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m optimizer.step()\n\u001b[32m     36\u001b[39m train_loss += loss.item() * x.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\Documents\\GitHub\\Audio-Transformer\\Spectrogram Transformer\\.venv\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\Documents\\GitHub\\Audio-Transformer\\Spectrogram Transformer\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\Documents\\GitHub\\Audio-Transformer\\Spectrogram Transformer\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "  model = MyViT((1,200,200), n_patches=25, n_blocks=4, hidden_d=32, n_heads=4, out_d=10).to(device)\n",
    "  model.load_state_dict(torch.load('ASTmodel_E10_200_25_4_4_25_32.pth', weights_only=True))\n",
    "\n",
    "  model.eval()\n",
    "  \n",
    "  test_set = SpeechCommands.CustomSpeechCommandsDataset(\"../datasets/custom_speech_commands\", subset=\"testing\", shuffle=True,divisor=30, out_size=(200,200))\n",
    "  test_loader = DataLoader(test_set, shuffle=False, batch_size=30)\n",
    "\n",
    "  criterion = CrossEntropyLoss()\n",
    "\n",
    "  # Test loop\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "      correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "      total += len(x)\n",
    "      \n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#  # Current model\n",
    "#  model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10,).to(device)\n",
    "#  model.load_state_dict(torch.load('my_model3_2_Transfer.pth',map_location=torch.device('cpu')))\n",
    "#\n",
    "#  #imgs = torch.randn(7, 1, 28, 28).to(device) # Dummy images\n",
    "#  #print(model(imgs).shape) # torch.Size([7, 49, 16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#  train_set = SpeechCommands.CustomSpeechCommandsDataset(\"../datasets/custom_speech_commands\",out_size=(28,28))\n",
    "#  _, label = train_set[0]\n",
    "#  print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
