{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0+cu126\n"
     ]
    }
   ],
   "source": [
    "# Visual Transformer based off the original publication An Image is Worth 16x16 Words:\n",
    "# https://arxiv.org/pdf/2010.11929\n",
    "\n",
    "# Code is taken from a tutorial by Brian Pulfer\n",
    "# https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "# Andrei Cartera -- Oct 2024\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import CustomSpeechCommands as SpeechCommands\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "CHW = (1,224,224)\n",
    "NUM_CLASSES = 10\n",
    "N_PATCHES = 28\n",
    "N_HEADS = 8\n",
    "N_ENCODERS = 4\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 16\n",
    "\n",
    "DROPOUT = 0.05\n",
    "ACTIVATION=\"gelu\"\n",
    "LR = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "  n, c, h, w = images.shape\n",
    "\n",
    "  assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "  patches = torch.zeros(n, n_patches ** 2, h * w * c // n_patches ** 2)\n",
    "  patch_size = h // n_patches\n",
    "\n",
    "  for idx, image in enumerate(images):\n",
    "    for i in range(n_patches):\n",
    "      for j in range(n_patches):\n",
    "        patch = image[:, i * patch_size: (i + 1) * patch_size, j * patch_size: (j + 1) * patch_size]\n",
    "        patches[idx, i * n_patches + j] = patch.flatten()\n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_patchify(images, n_patches):\n",
    "  n, c, h, w = images.shape\n",
    "  assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "  patch_size = h // n_patches\n",
    "  patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "  patches = patches.contiguous().view(n, c, -1, patch_size, patch_size)\n",
    "  patches = patches.permute(0, 2, 1, 3, 4).contiguous().view(n, -1, patch_size * patch_size * c)\n",
    "  \n",
    "  return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "  result = torch.ones(sequence_length, d)\n",
    "  for i in range(sequence_length):\n",
    "    for j in range(d):\n",
    "      result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 625, 64])\n"
     ]
    }
   ],
   "source": [
    "image = torch.rand(1,1,200,200)\n",
    "patches = patchify(image, 25)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads=2):\n",
    "    super(MyMSA, self).__init__()\n",
    "    self.d = d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "    d_head = int(d / n_heads)\n",
    "    self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.d_head = d_head\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    # Sequences has shape (N, seq_length, token_dim)\n",
    "    # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "    # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "      seq_result = []\n",
    "      for head in range(self.n_heads):\n",
    "        q_mapping = self.q_mappings[head]\n",
    "        k_mapping = self.k_mappings[head]\n",
    "        v_mapping = self.v_mappings[head]\n",
    "\n",
    "        seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "        q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "        attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "        seq_result.append(attention @ v)\n",
    "      result.append(torch.hstack(seq_result))\n",
    "    return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "  def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "    super(MyViTBlock, self).__init__()\n",
    "    self.hidden_d = hidden_d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(hidden_d)\n",
    "    self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "    self.norm2 = nn.LayerNorm(hidden_d)\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mhsa(self.norm1(x))\n",
    "    out = out + self.mlp(self.norm2(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViT(nn.Module):\n",
    "  def __init__(self, chw, n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "    # Super constructor\n",
    "    super(MyViT, self).__init__()\n",
    "    \n",
    "    # Attributes\n",
    "    self.chw = chw # ( C , H , W )\n",
    "    self.n_patches = n_patches\n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_heads = n_heads\n",
    "    self.hidden_d = hidden_d\n",
    "    \n",
    "    # Input and patches sizes\n",
    "    assert chw[1] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    assert chw[2] % n_patches == 0, \"Input shape not entirely divisible by number of patches\"\n",
    "    self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "    # 1) Linear mapper\n",
    "    self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "    \n",
    "    # 2) Learnable classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "    \n",
    "    # 3) Positional embedding\n",
    "    self.register_buffer('positional_embeddings', get_positional_embeddings(n_patches ** 2 + 1, hidden_d), persistent=False)\n",
    "    \n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "    \n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self.hidden_d, out_d),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, images):\n",
    "    # Dividing images into patches\n",
    "    n, c, h, w = images.shape\n",
    "    patches = optimized_patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    tokens = self.linear_mapper(patches)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "        \n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    \n",
    "    return self.mlp(out) # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Loading data\n",
    "\n",
    "  print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "  model = MyViT(CHW, N_PATCHES, N_ENCODERS, HIDDEN_DIM, N_HEADS, NUM_CLASSES).to(device)\n",
    "  #model.load_state_dict(torch.load('my_model3_2_Transfer.pth',map_location=torch.device('cpu')))\n",
    "  #https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "  train_set = SpeechCommands.CustomSpeechCommandsDataset(\"../datasets/custom_speech_commands\", shuffle=True, divisor=BATCH_SIZE, out_size=(224,224))\n",
    "  test_set = SpeechCommands.CustomSpeechCommandsDataset(\"../datasets/custom_speech_commands\", subset=\"testing\", shuffle=True,divisor=BATCH_SIZE, out_size=(224,224))\n",
    "  \n",
    "  train_loader = DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "  test_loader = DataLoader(test_set, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "  # Defining model and training options\n",
    "\n",
    "  # Training loop\n",
    "  optimizer = Adam(model.parameters(), lr=LR)\n",
    "  criterion = CrossEntropyLoss()\n",
    "  #scheduler = lr_scheduler.LinearLR(optimizer)\n",
    "  scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "                                             \n",
    "  for epoch in trange(EPOCHS, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "      x, y = batch\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item() * x.size(0)\n",
    "      \n",
    "    train_loss /= len(train_loader.dataset) \n",
    "    scheduler.step(train_loss)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "  torch.save(model.state_dict(), 'ASTmodel_E30_224_28_8_4_32_16.pth')\n",
    "\n",
    "  # Test loop\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "      correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "      total += len(x)\n",
    "      \n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":  \n",
    "  model = MyViT((1,200,200), n_patches=25, n_blocks=4, hidden_d=32, n_heads=4, out_d=10).to(device)\n",
    "  model.load_state_dict(torch.load('ASTmodel_E10_200_25_4_4_25_32.pth', weights_only=True))\n",
    "\n",
    "  test_set = SpeechCommands.CustomSpeechCommandsDataset(\"../datasets/custom_speech_commands\", subset=\"testing\", shuffle=True,divisor=30, out_size=(200,200))\n",
    "  test_loader = DataLoader(test_set, shuffle=False, batch_size=30)\n",
    "\n",
    "  criterion = CrossEntropyLoss()\n",
    "\n",
    "  # Test loop\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "      correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "      total += len(x)\n",
    "      \n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#  # Current model\n",
    "#  model = MyViT((1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10,).to(device)\n",
    "#  model.load_state_dict(torch.load('my_model3_2_Transfer.pth',map_location=torch.device('cpu')))\n",
    "#\n",
    "#  #imgs = torch.randn(7, 1, 28, 28).to(device) # Dummy images\n",
    "#  #print(model(imgs).shape) # torch.Size([7, 49, 16])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == '__main__':\n",
    "#  train_set = SpeechCommands.CustomSpeechCommandsDataset(\"../datasets/custom_speech_commands\",out_size=(28,28))\n",
    "#  _, label = train_set[0]\n",
    "#  print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
