{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Description: This script trains an AST whose input has been modified to take audio insteasd of patches of images \n",
    "# Original code is based off a tutorial by Brian Pulfer\n",
    "# https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "# Andrei Cartera -- Mar 2025\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import CustomSpeechCommands_R as SpeechCommands\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, lr_scheduler\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Hyperparameters\n",
    "N_SEGMENTS = 32\n",
    "REPC_VEC_SIZE = 40\n",
    "\n",
    "EPOCHS = 30\n",
    "N_HEADS = 8\n",
    "N_ENCODERS = 4\n",
    "BATCH_SIZE = 32\n",
    "HIDDEN_DIM = 16\n",
    "DROPOUT = 0.05\n",
    "ACTIVATION=\"gelu\"\n",
    "LR = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_positional_embeddings(sequence_length, d):\n",
    "  result = torch.ones(sequence_length, d)\n",
    "  for i in range(sequence_length):\n",
    "    for j in range(d):\n",
    "      result[i][j] = np.sin(i / (10000 ** (j / d))) if j % 2 == 0 else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMSA(nn.Module):\n",
    "  def __init__(self, d, n_heads=2):\n",
    "    super(MyMSA, self).__init__()\n",
    "    self.d = d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    assert d % n_heads == 0, f\"Can't divide dimension {d} into {n_heads} heads\"\n",
    "\n",
    "    d_head = int(d / n_heads)\n",
    "    self.q_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.k_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.v_mappings = nn.ModuleList([nn.Linear(d_head, d_head) for _ in range(self.n_heads)])\n",
    "    self.d_head = d_head\n",
    "    self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "  def forward(self, sequences):\n",
    "    # Sequences has shape (N, seq_length, token_dim)\n",
    "    # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "    # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "    result = []\n",
    "    for sequence in sequences:\n",
    "      seq_result = []\n",
    "      for head in range(self.n_heads):\n",
    "        q_mapping = self.q_mappings[head]\n",
    "        k_mapping = self.k_mappings[head]\n",
    "        v_mapping = self.v_mappings[head]\n",
    "\n",
    "        seq = sequence[:, head * self.d_head: (head + 1) * self.d_head]\n",
    "        q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "\n",
    "        attention = self.softmax(q @ k.T / (self.d_head ** 0.5))\n",
    "        seq_result.append(attention @ v)\n",
    "      result.append(torch.hstack(seq_result))\n",
    "    return torch.cat([torch.unsqueeze(r, dim=0) for r in result])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyViTBlock(nn.Module):\n",
    "  def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "    super(MyViTBlock, self).__init__()\n",
    "    self.hidden_d = hidden_d\n",
    "    self.n_heads = n_heads\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(hidden_d)\n",
    "    self.mhsa = MyMSA(hidden_d, n_heads)\n",
    "    self.norm2 = nn.LayerNorm(hidden_d)\n",
    "    self.mlp = nn.Sequential(\n",
    "      nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "      nn.GELU(),\n",
    "      nn.Linear(mlp_ratio * hidden_d, hidden_d)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = x + self.mhsa(self.norm1(x))\n",
    "    out = out + self.mlp(self.norm2(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioTransformer(nn.Module):\n",
    "  def __init__(self, n_segments, repc_vec_size , n_blocks=2, hidden_d=8, n_heads=2, out_d=10):\n",
    "    # Super constructor\n",
    "    super(AudioTransformer, self).__init__()\n",
    "    \n",
    "    # Attributes\n",
    "    self.n_segments = n_segments\n",
    "    self.repc_vec_size  = repc_vec_size \n",
    "    self.n_blocks = n_blocks\n",
    "    self.n_heads = n_heads\n",
    "    self.hidden_d = hidden_d\n",
    "    \n",
    "    # 1) Linear mapper\n",
    "    self.input_d = repc_vec_size\n",
    "    self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "    \n",
    "    # 2) Learnable classification token\n",
    "    self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "    \n",
    "    # 3) Positional Embedding\n",
    "    self.register_buffer(\n",
    "      'positional_embeddings',\n",
    "      get_positional_embeddings(n_segments + 1, hidden_d),\n",
    "      persistent=False\n",
    "    )\n",
    "    # 4) Transformer encoder blocks\n",
    "    self.blocks = nn.ModuleList([MyViTBlock(hidden_d, n_heads) for _ in range(n_blocks)])\n",
    "    \n",
    "    # 5) Classification MLPk\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(self.hidden_d, out_d),\n",
    "        nn.Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, audio):\n",
    "    \n",
    "    # Running linear layer tokenization\n",
    "    # Map the vector corresponding to each patch to the hidden size dimension\n",
    "    tokens = self.linear_mapper(audio)\n",
    "    \n",
    "    # Adding classification token to the tokens\n",
    "    tokens = torch.cat((self.class_token.expand(audio.shape[0], 1, -1), tokens), dim=1)\n",
    "    \n",
    "    # Adding positional embedding\n",
    "    out = tokens + self.positional_embeddings.repeat(audio.shape[0], 1, 1)\n",
    "    \n",
    "    # Transformer Blocks\n",
    "    for block in self.blocks:\n",
    "        out = block(out)\n",
    "        \n",
    "    # Getting the classification token only\n",
    "    out = out[:, 0]\n",
    "    \n",
    "    return self.mlp(out) # Map to output dimension, output category distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "  # Loading data\n",
    "\n",
    "  print(\"Using device: \", device, f\"({torch.cuda.get_device_name(device)})\" if torch.cuda.is_available() else \"\")\n",
    "  model = AudioTransformer(N_SEGMENTS, REPC_VEC_SIZE, N_ENCODERS, HIDDEN_DIM, N_HEADS, NUM_CLASSES).to(device)\n",
    "  #model.load_state_dict(torch.load('my_model3_2_Transfer.pth',map_location=torch.device('cpu')))\n",
    "  #https://pytorch.org/tutorials/beginner/basics/data_tutorial.html\n",
    "\n",
    "  train_set = SpeechCommands.CustomSpeechCommandsDataset_R(\"../custom_speech_commands\", shuffle=True, vec_size=REPC_VEC_SIZE, divisor=BATCH_SIZE,)\n",
    "  test_set = SpeechCommands.CustomSpeechCommandsDataset_R(\"../custom_speech_commands\", subset=\"testing\", shuffle=True, vec_size=REPC_VEC_SIZE,divisor=BATCH_SIZE,)\n",
    "  \n",
    "  train_loader = DataLoader(train_set, shuffle=True, batch_size=BATCH_SIZE)\n",
    "  test_loader = DataLoader(test_set, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "  # Defining model and training options\n",
    "\n",
    "  # Training loop\n",
    "  optimizer = Adam(model.parameters(), lr=LR)\n",
    "  criterion = CrossEntropyLoss()\n",
    "  #scheduler = lr_scheduler.LinearLR(optimizer)\n",
    "  scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "  model.train()  # Set the model to training mode                                     \n",
    "  for epoch in trange(EPOCHS, desc=\"Training\"):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False):\n",
    "      x, y = batch\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      train_loss += loss.item() * x.size(0)\n",
    "      \n",
    "    train_loss /= len(train_loader.dataset) \n",
    "    scheduler.step(train_loss)\n",
    "    #torch.cuda.empty_cache()\n",
    "    \n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} loss: {train_loss:.2f}, LR: {current_lr}\")\n",
    "\n",
    "\n",
    "  torch.save(model.state_dict(), 'models/ATmodel_32SEG_40VEC_E30_8_4_B32_H16.pth')\n",
    "\n",
    "  # Test loop\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "\n",
    "      batch_size = x.size(0)\n",
    "      total_test_loss += loss.item() * batch_size\n",
    "      total_samples += batch_size\n",
    "\n",
    "    average_test_loss = total_test_loss / total_samples\n",
    "    print(f\"Test loss: {average_test_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:  cuda:0 (NVIDIA GeForce RTX 4060)\n",
      "Balanced dataset to 3296 samples per label.\n",
      "Balanced dataset to 384 samples per label.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrei\\Documents\\dP\\svn\\MUS471-471L\\Project\\Source\\.venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b1730221aa40feac47c24d2fb3e72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "595384dc8e96438da846b2eb9cab2ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 in training:   0%|          | 0/1030 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 34\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     31\u001b[39m loss = criterion(y_hat, y)\n\u001b[32m     33\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m optimizer.step()\n\u001b[32m     37\u001b[39m train_loss += loss.item() * x.size(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\Documents\\dP\\svn\\MUS471-471L\\Project\\Source\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\Documents\\dP\\svn\\MUS471-471L\\Project\\Source\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Andrei\\Documents\\dP\\svn\\MUS471-471L\\Project\\Source\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
