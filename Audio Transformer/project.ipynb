{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "GetBCoeffs\n",
    "\n",
    "#### Project Progress :\n",
    "\n",
    "1. ~~find zero-crossings~~\n",
    "2. ~~find cycles~~\n",
    "3. ~~choose representative cycles based on biases~~\n",
    "4. form a model with bcoeffs for each representative cycle\n",
    "5. interpolate between each cycle using bcoeffs for amplitude shape and glissando for pitch\n",
    "6. test accuracy to original sound\n",
    "7. Tokenize and train model\n",
    "8. Profit?\n",
    "   \n",
    "#### Ideas\n",
    "\n",
    "Test with no interpolation between the representative cycles or by sample-and-hold the cycles for a duration\n",
    "\n",
    "What if instead of tokenizing the b-spline coefficients we use the overtones or harmonics of each cycle? At the end of the day, we know that a neural network can distinguish pitch visually.\n",
    "\n",
    "What if we tokenize the information of the differencs in the b-splines\n",
    "\n",
    "To better find cycles: group cycles by similarity by using a dot product, larger dot products are much more similar. Find one from the largest group as your repcycle. Make a Union of the groups to find the group whose cycles covers the whole segment.\n",
    "\n",
    "Compare cycles of next segment with the repcycle of the previous "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['soundfile']\n"
     ]
    }
   ],
   "source": [
    "# Testing for Pytorch Audio prcessing\n",
    "# Andrei Cartera \n",
    "import os\n",
    "import numpy as np\n",
    "#from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import torchaudio.transforms as T\n",
    "import torch.nn.functional as nnF\n",
    "#from IPython.display import Image\n",
    "import heapq\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torchaudio \n",
    "\n",
    "# Install soundfile and ffmpeg-python if not already installed\n",
    "# pip install soundfile\n",
    "# pip install ffmpeg-python\n",
    "\n",
    "#print(torch.__version__)\n",
    "print(str(torchaudio.list_audio_backends()))\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUDIO_PATH = \"./audio/\"\n",
    "FILE_NAME = \"zero.wav\"\n",
    "AUDIO_SIZE = 16000\n",
    "SEGMENT_LENGTH = 500\n",
    "SEGMENT_NUM = 8\n",
    "NSR_FACTOR = 3.14\n",
    "CYCLE_EPSILON = 40\n",
    "FFT_SIZE = 1024\n",
    "\n",
    "SHOW_WAV = False\n",
    "SHOW_FFT = False\n",
    "SHOW_REPC = False\n",
    "\n",
    "F0_FREQ_RANGE = (50,200)\n",
    "\n",
    "# how much to conisder frequency, loudness, or middleness, higher values means less adherence\n",
    "F_SCORE_WEIGHT = 1 # Frequency \n",
    "R_SCORE_WEIGHT = 1 # RMS loudness\n",
    "M_SCORE_WEIGHT = 1 # Middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT EDIT\n",
    "LOW_F0_INDEX = math.floor(FFT_SIZE/AUDIO_SIZE * F0_FREQ_RANGE[0])\n",
    "HIGH_F0_INDEX = math.floor(FFT_SIZE/AUDIO_SIZE * F0_FREQ_RANGE[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions and Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets k largest elements from a list along with its indices \n",
    "def klargest_with_indices(arr, k, range=None): \n",
    "  heap = []\n",
    "  for i, val in enumerate(arr):\n",
    "      if range and not (range[0] <= i <= range[1]):\n",
    "        continue\n",
    "      heap.append((i,val))\n",
    "  return heapq.nlargest(k, heap, key=lambda x: x[1])\n",
    "\n",
    "def calculate_rms(audio):\n",
    "  return np.sqrt(np.mean(audio**2))\n",
    "\n",
    "def find_factor_pairs(n):\n",
    "  factor_pairs = []\n",
    "  for i in range(1, int(n**0.5) + 1):\n",
    "    if n % i == 0:\n",
    "      factor_pairs.append((i, int(n // i)))\n",
    "  return factor_pairs\n",
    "\n",
    "# returns an array of RMS values from a signal over time specified by the window size \n",
    "def rms_over_windows(waveform_arr, window_size=100, silence_threshold = None):\n",
    "  rms_values = []\n",
    "  excluded_ranges = []\n",
    "  \n",
    "  start = 0\n",
    "  while start < len(waveform_arr):\n",
    "    end = start + window_size\n",
    "    if end > len(waveform_arr):\n",
    "      end = len(waveform_arr)\n",
    "    window = waveform_arr[start:end]\n",
    "    rms_value = calculate_rms(window)\n",
    "    rms_values.append(rms_value)\n",
    "    \n",
    "    if silence_threshold:\n",
    "      if rms_value < silence_threshold:\n",
    "        excluded_ranges.append((start, end))\n",
    "\n",
    "    start = end\n",
    "  \n",
    "  return rms_values, excluded_ranges\n",
    "\n",
    "def resize_spectrogram(spectrogram, target_size=(224, 224)):\n",
    "  # Interpolate to the target size\n",
    "  resized_spectrogram = nnF.interpolate(spectrogram.unsqueeze(0), size=target_size, mode='bilinear', align_corners=False)\n",
    "  return resized_spectrogram.squeeze(0)\n",
    "\n",
    "def show_fft(fft_mag, start_sample:int, top:int=0):\n",
    "  from matplotlib.ticker import ScalarFormatter\n",
    "  \n",
    "  # make x-axis frequencies\n",
    "  frequencies = np.linspace(0, AUDIO_SIZE / 2, FFT_SIZE // 2)\n",
    "  out_path = f\".\\\\figures\\\\fft_{FILE_NAME.split('.')[0]}{start_sample}.png\"\n",
    "  # Convert magnitude to dB\n",
    "  #fft_dB = 20 * np.log10((2/SEGMENT_LENGTH) * fft_mag)\n",
    "\n",
    "  # Plot the FFT magnitude as a bar plot\n",
    "  fig = plt.figure(figsize=(40, 10))\n",
    "  ax = fig.add_subplot(111)\n",
    "  #plt.plot(frequencies, fft_dB)\n",
    "  plt.bar(frequencies, fft_mag, width=15, align='center', alpha=0.75)\n",
    "  plt.xscale('log')\n",
    "  plt.xticks([50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000, 1500, 2000, 3000, 4000, 5000, 8000 ],\n",
    "             ['50','100','200','300','400','500','600','700','800','900','1000','1500','2000','3000','4000','5000','8000']) \n",
    "  plt.xticks(rotation=45)  # Rotate by 45 degrees\n",
    "  plt.gca().xaxis.set_major_formatter(ScalarFormatter())\n",
    "  plt.title(f\"FFT of \\\"{FILE_NAME}\\\" from {start_sample} to {start_sample+SEGMENT_LENGTH} samples\")\n",
    "  plt.xlabel(\"Frequency (Hz)\")\n",
    "  plt.ylabel(\"Magnitude\")\n",
    "  if(top):\n",
    "    largest = klargest_with_indices(fft_mag, top, (LOW_F0_INDEX, HIGH_F0_INDEX))\n",
    "    for index,value in largest:\n",
    "      max_x = AUDIO_SIZE/FFT_SIZE * index\n",
    "      max_y = value\n",
    "      ax.plot(max_x, max_y, 'ro')\n",
    "      ax.annotate(f'{max_x} Hz', xy=(max_x, max_y), xytext=(max_x+max_x/15, max_y),color='red')\n",
    "  plt.grid(True)\n",
    "  plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "  plt.close()\n",
    "  # Open the saved image with the default viewer\n",
    "  if SHOW_WAV:\n",
    "    if os.path.exists(out_path):\n",
    "      if os.name == 'nt':  # For Windows\n",
    "        os.startfile(out_path)\n",
    "\n",
    "  # Plot the waveform\n",
    "def show_waveform(waveform_arr, start_sample, zero_crossings:np.array, repcycle=None):\n",
    "\n",
    "  out_path = f\".\\\\figures\\\\{FILE_NAME.split('.')[0]}{start_sample}.png\"\n",
    "\n",
    "  fig = plt.figure(figsize=(100, 10))\n",
    "  ax = fig.add_subplot(111)\n",
    "  plt.plot(range(start_sample, start_sample+len(waveform_arr)), waveform_arr)\n",
    "  y_max = max(abs(np.amin(waveform_arr)), abs(np.amax(waveform_arr)))\n",
    "\n",
    "  if y_max < 0.1: \n",
    "    y_max = 0.1\n",
    "    plt.ylim(-y_max, y_max)\n",
    "  if np.size(zero_crossings, 0) > 0:\n",
    "    plt.vlines(zero_crossings, -y_max, y_max, colors='red', linestyles=\"dashed\", linewidth=1)\n",
    "\n",
    "  if(repcycle):\n",
    "    #ax.fill_between(t, 1, where=s > 0, facecolor='green', alpha=.5)\n",
    "    #ax.fill_between(t, -1, where=s < 0, facecolor='red', alpha=.5)\n",
    "    ax.fill_between(repcycle, -y_max, y_max, facecolor='green', alpha=.25)\n",
    "  plt.xticks(np.arange(start_sample, start_sample+len(waveform_arr), 100))\n",
    "  plt.title(f\"Waveform of \\\"{FILE_NAME}\\\"\")\n",
    "  plt.xlabel(\"Samples\")\n",
    "  plt.ylabel(\"Amplitude\")\n",
    "  plt.grid()\n",
    "  plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "  plt.close()\n",
    "  # Open the saved image with the default viewer\n",
    "  if SHOW_FFT:\n",
    "    if os.path.exists(out_path):\n",
    "      if os.name == 'nt':  # For Windows\n",
    "        os.startfile(out_path)\n",
    "\n",
    "def fft_max(waveform_arr : np.array):\n",
    "\n",
    "  # Pad with zeros if necessary \n",
    "  if(len(waveform_arr) != FFT_SIZE):\n",
    "    size_diff = abs(len(waveform_arr)-FFT_SIZE)\n",
    "    list(waveform_arr).extend([0] * size_diff)\n",
    "\n",
    "    # perform fft and get magnitude\n",
    "  fft = np.fft.fft(waveform_arr, FFT_SIZE)\n",
    "  fft_mag = np.abs(fft)\n",
    "  \n",
    "    #turn back to np.array\n",
    "  fft_mag = fft_mag[:FFT_SIZE // 2]\n",
    "  max_index = np.argmax(fft_mag)\n",
    "  return fft_mag, max_index\n",
    "  \n",
    "def show_repcycles(repcycles, waveform_arr):\n",
    "  # Find a pair of multiples of n_segments for dysplaing grid of waveforms \n",
    "  n_segments = AUDIO_SIZE / SEGMENT_LENGTH\n",
    "  min_diff = np.inf\n",
    "  row_col_pair = None\n",
    "  for pair in find_factor_pairs(n_segments):\n",
    "    diff = abs(pair[0]-pair[1])\n",
    "    if(diff < min_diff):\n",
    "      row_col_pair = pair\n",
    "      min_diff = diff\n",
    "  nrows, ncols = pair\n",
    "\n",
    "  ### make grid plot\n",
    "  _, axes = plt.subplots(nrows, ncols, figsize=(100, 50))\n",
    "\n",
    "  axes = axes.flatten()\n",
    "  # Loop through the data and corresponding axes to plot\n",
    "  for i, ax in enumerate(axes):\n",
    "    if i < len(repcycles):\n",
    "\n",
    "      if repcycles[i]:\n",
    "        \n",
    "        curr_repcycle = repcycles[i]\n",
    "        repc_start = math.floor(curr_repcycle[0])\n",
    "        repc_end = math.floor(curr_repcycle[1])\n",
    "        ticks = np.linspace(repc_start, repc_end, 10)\n",
    "        ax.plot(range(repc_start, repc_end), waveform_arr[repc_start:repc_end])\n",
    "        ax.set_xticks(ticks)\n",
    "        y_max = max(abs(np.amin(repcycles[i])), abs(np.amax(repcycles[i])))   \n",
    "\n",
    "        if y_max < 0.1: \n",
    "          y_max = 0.1\n",
    "          plt.ylim(-y_max, y_max)\n",
    "\n",
    "        ax.grid(True)\n",
    "\n",
    "      else:\n",
    "        ax.text(0.5, 0.5, 'NONE', fontsize=50, ha='center', va='center', alpha=0.5)\n",
    "    else:\n",
    "      ax.axis('off') \n",
    "\n",
    "  out_path = f\".\\\\figures\\\\{FILE_NAME}_repc_segmentsize{SEGMENT_LENGTH}.png\"\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.savefig(out_path, bbox_inches='tight', pad_inches=0)\n",
    "  plt.close()\n",
    "  # Open the saved image with the default viewer\n",
    "  if SHOW_REPC:\n",
    "    if os.path.exists(out_path):\n",
    "      if os.name == 'nt':  # For Windows\n",
    "        os.startfile(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_zerocrossings(waveform_arr:np.array, start_sample, dynamic_threshold=0.01):\n",
    "  last=None \n",
    "  zerocrossings = np.array([], dtype='f')\n",
    "  \n",
    "  #Filter out quiet parts of the signal using a RMS with windowing\n",
    "  _, excluded_ranges = rms_over_windows(waveform_arr, 100, dynamic_threshold)\n",
    "\n",
    "  # turn the excluded ranges into an array of samples that shouldn't be processed\n",
    "  excluded_samples = [] \n",
    "  for start, end in excluded_ranges:\n",
    "    excluded_samples.extend(range(start_sample + start, start_sample + end))\n",
    "\n",
    "  #sweep the whole segment\n",
    "  for index, value in enumerate(waveform_arr):\n",
    "    absolute_index = start_sample + index\n",
    "    if absolute_index in excluded_samples:\n",
    "      continue \n",
    "    if last == None:  #record last sample\n",
    "      last = value\n",
    "    #elif value == 0:\n",
    "    #  if np.sign(last) == -1.0 and np.sign(waveform_arr[index+1]) == 1.0:\n",
    "    #    np.append(zerocrossings, start_sample+index)\n",
    "    \n",
    "    # if last sample is negative and current one is positive a zero crossing occured\n",
    "    if np.sign(last) == -1.0 and np.sign(value) == 1.0: \n",
    "      # x = x1 + (x2-x1)/(y2-y1)*(y-y1)\n",
    "      inter_x = (index-1) + (-last/(value-last))\n",
    "      zerocrossings = np.append(zerocrossings,start_sample+inter_x)\n",
    "    last = value\n",
    "  return zerocrossings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cycles_f0(f0, start_sample, zero_crossings:list):\n",
    "  \n",
    "  cycles = []\n",
    "  f0_length = 1.0/f0 * AUDIO_SIZE #length of samples of fundamental frequency\n",
    "  last_sample = start_sample*SEGMENT_LENGTH + SEGMENT_LENGTH \n",
    "  for start in zero_crossings:\n",
    "    cyclef0_end = start+f0_length # where we are expecting the cycle to end \n",
    "    \n",
    "    if cyclef0_end > last_sample+CYCLE_EPSILON: #don't look past end of segment, unless CYCLE_EPSILON allows you to see the end \n",
    "      continue\n",
    "\n",
    "    high = cyclef0_end + CYCLE_EPSILON #acceptabale ranges\n",
    "    low = cyclef0_end - CYCLE_EPSILON\n",
    "    \n",
    "    #get values that fall withing the f0+epsilon range\n",
    "    try:\n",
    "      possible_cycles = list(filter(lambda x, high=high, low=low: low <= x <= high ,zero_crossings))\n",
    "      if not possible_cycles:\n",
    "        #print(f\"Failed to find a cycle for sample {start} with an epsilon of {CYCLE_EPSILON}\")\n",
    "        continue\n",
    "      if len(possible_cycles) > 1:\n",
    "        # tie-breaker get the end sample that is closest to the expected\n",
    "        differences = list(map(lambda x, y=cyclef0_end: abs(y-x), possible_cycles))\n",
    "        end = possible_cycles[np.argmin(differences)]\n",
    "      else:\n",
    "        end = possible_cycles[0]\n",
    "    except Exception as e:\n",
    "      print(f\"Error: Failed to find a cycle for sample {start} with an epsilon of {CYCLE_EPSILON}\\n\",e)\n",
    "    cycles.append((start,end))\n",
    "  return cycles\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_repcycle(segment_wav, start_sample, f0, cycles):\n",
    "  #score the cycles based on they adherence to the following biases:\n",
    "  # 1. closeset to f0\n",
    "  # 2. loudness\n",
    "  # 3. most in the middle\n",
    "  mid_segment = start_sample+ (SEGMENT_LENGTH/2);\n",
    "  f0_length = 1.0/f0 * AUDIO_SIZE #length of samples of fundamental frequency\n",
    "  \n",
    "  #get length, RMS, and middle of all cycles \n",
    "  fRm_score = []\n",
    "  for index, x in enumerate(cycles):\n",
    "    #get length of the cycle and comapre with fundamental frequency reciprocal \n",
    "    freq = x[1] - x[0]\n",
    "    f_score = abs(freq - f0_length)\n",
    "\n",
    "    #get middle and comare with middle of the whole segment  \n",
    "    middle = (x[0] + x[1]) / 2\n",
    "    m_score = abs(middle - mid_segment)\n",
    "\n",
    "    #get rms\n",
    "    i_beginning = math.floor(x[0]-start_sample)\n",
    "    i_end = math.floor(x[1]-start_sample)\n",
    "    r_score = 1-calculate_rms(segment_wav[i_beginning:i_end])\n",
    "\n",
    "    fRm_score.append((index, f_score*F_SCORE_WEIGHT, r_score*R_SCORE_WEIGHT, m_score*M_SCORE_WEIGHT))\n",
    "\n",
    "  sorted_list = sorted(fRm_score, key=lambda x: x[1]+x[2]+x[3])\n",
    "  return cycles[sorted_list[0][0]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "waveform, sample_rate = torchaudio.load(AUDIO_PATH + FILE_NAME)\n",
    "waveform_arr = waveform.t().squeeze(1).numpy()\n",
    "\n",
    "#identify SNR reciprocal to get a dynamic noise threshold for filtering \n",
    "rms_values, _ = rms_over_windows(waveform_arr, SEGMENT_LENGTH) #calculate RMS over time\n",
    "\n",
    "# Get Noise-to-Signal Ratio\n",
    "signal_rms = np.max(rms_values)\n",
    "noise_rms = np.min(rms_values)\n",
    "nsr = noise_rms/ signal_rms\n",
    "\n",
    "# Calculate dynamic silence threshold based on NSR\n",
    "silence_threshold = nsr*NSR_FACTOR\n",
    "\n",
    "# Split the waveform into segments\n",
    "split_waveform = np.array_split(waveform_arr, np.arange(SEGMENT_LENGTH, AUDIO_SIZE, SEGMENT_LENGTH))\n",
    "#split_waveform = [waveform_np[i:i+segment_length] for i in range(0, 16000, segment_length)]\n",
    "\n",
    "####### TESTING - get one segment ############\n",
    "#segment_wav = split_waveform[SEGMENT_NUM]\n",
    "#start_sample = SEGMENT_NUM*SEGMENT_LENGTH\n",
    "########################################\n",
    "\n",
    "repcycles = []\n",
    "\n",
    "for segment_num, segment_wav in enumerate(split_waveform):\n",
    "  #segment_wav = split_waveform[SEGMENT_NUM]\n",
    "  start_sample = segment_num*SEGMENT_LENGTH\n",
    "  # Get the Zero-Crossings, ignoring noise\n",
    "  zero_crossings = find_zerocrossings(segment_wav, start_sample, silence_threshold)\n",
    "  if not len(zero_crossings) > 1:\n",
    "    repcycles.append([])\n",
    "    continue\n",
    "  # Get the fundamental frequency using the FFT\n",
    "  fft_mag, max_index = fft_max(segment_wav)\n",
    "\n",
    "  # exclude bins outside the F0_FREQ_RANGE\n",
    "  # get frequency to bin index\n",
    "\n",
    "  top_bins = klargest_with_indices(fft_mag, 4, (LOW_F0_INDEX,HIGH_F0_INDEX))\n",
    "  assert len(top_bins) > 0, \"Error: failed to find a fundamental frequency\"\n",
    "  #top_in_range = list(filter(lambda x, h=LOW_F0, l = HIGH_F0: l <= x[0] <= h, top_bins))[0]\n",
    "\n",
    "  # Fundamental Frequency: Sample Rate / fftsize * fft_bin_index\n",
    "  f0 = AUDIO_SIZE/FFT_SIZE * top_bins[0][0]\n",
    "\n",
    "  #print(f\"silence_threshold: {silence_threshold}\")\n",
    "  #print(f\"f0: {f0}\")\n",
    "  #print(f\"F0 magnitude{fft_mag[max_index]}\")\n",
    "\n",
    "  #find cycles within the signal and find a representative one for the segment\n",
    "  cycles = find_cycles_f0(f0, start_sample, zero_crossings)\n",
    "  if not len(cycles) > 0:\n",
    "    repcycles.append([])\n",
    "    continue\n",
    "\n",
    "  repcycle = find_repcycle(segment_wav, start_sample, f0, cycles)\n",
    "  assert repcycle != None, \"Error: failed to find a representative cycle\"\n",
    "\n",
    "  repcycles.append(repcycle)\n",
    "  #show_waveform(segment_wav, start_sample, zero_crossings, repcycle)\n",
    "  #show_fft(fft_mag, start_sample, 4)\n",
    "\n",
    "show_repcycles(repcycles, waveform_arr)\n",
    "\n",
    "#for num in cycles:\n",
    "#  print(f\"({num[0]:.2f}, {num[1]:.2f})\")\n",
    "#with open('fft_freq.txt', 'w') as file:\n",
    "#  for index, value in enumerate(fft_mag):\n",
    "#    file.write(f\"{value} at {index*AUDIO_SIZE/FFT_SIZE}\")\n",
    "\n",
    "#### TEST- output audio 10 copies of repcycle ####\n",
    "outfile_wav = []\n",
    "for curr_repcycle in repcycles:\n",
    "  if curr_repcycle:\n",
    "    repc_start = math.floor(curr_repcycle[0])\n",
    "    repc_end = math.floor(curr_repcycle[1])\n",
    "    rep_cycle_wav = waveform_arr[repc_start:repc_end]\n",
    "    for _ in range(10):\n",
    "      outfile_wav.extend(rep_cycle_wav)\n",
    "    \n",
    "outfile_tensor = torch.tensor(outfile_wav).unsqueeze(0)\n",
    "torchaudio.save(f\"{FILE_NAME.split(\".\")[0]}_out.wav\", outfile_tensor, 16000)\n",
    "################################################################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
