{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "cuda:0\n",
      "2.6.0+cu126\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Description: This script trains an AST whose input has been modified to take audio insteasd of patches of images \n",
    "# Original code is based off a tutorial by Brian Pulfer\n",
    "# https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "# Andrei Cartera -- Mar 2025\n",
    "\n",
    "import numpy as np\n",
    "import CustomSpeechCommands_R as SpeechCommands\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import DataLoader\n",
    "from AudioTransformer import AudioTransformer\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Hyperparameters\n",
    "N_SEGMENTS = 32\n",
    "REPC_VEC_SIZE = 64\n",
    "\n",
    "N_HEADS = 8\n",
    "N_ENCODERS = 4\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 32\n",
    "DROPOUT = 0.15\n",
    "ACTIVATION=\"gelu\"\n",
    "LR = 0.0009\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/N_ATmodel_32SEG_64VEC_E100_8_4_B64_H32_2.pth\n",
      "Using repcycles from: ..\\custom_speech_commands_repcycles32\n",
      "Balanced dataset to 384 samples per label.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35dc151044614ad0990c5b0fac677283",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing:   0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Andrei\\Documents\\dP\\svn\\MUS471-471L\\Project\\Source\\CustomSpeechCommands_R.py:231: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  out[i] = torch.tensor(repc_wav)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.73\n",
      "Test accuracy: 73.15%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "  MODEL_PATH = \"models/N_ATmodel_32SEG_64VEC_E100_8_4_B64_H32_Control.pth\"\n",
    "\n",
    "  model = AudioTransformer(N_SEGMENTS, REPC_VEC_SIZE, N_ENCODERS, HIDDEN_DIM, N_HEADS, NUM_CLASSES).to(device)\n",
    "  model.load_state_dict(torch.load(MODEL_PATH, weights_only=True))\n",
    "  print(f\"Model loaded from {MODEL_PATH}\")\n",
    "  \n",
    "  model.eval()  # Set the model to evaluation mode\n",
    "  \n",
    "  test_set = SpeechCommands.CustomSpeechCommandsDataset_R(\"../custom_speech_commands\", n_segments=N_SEGMENTS, subset=\"testing\", shuffle=True, vec_size=REPC_VEC_SIZE, divisor=BATCH_SIZE)\n",
    "  test_loader = DataLoader(test_set, shuffle=False, batch_size=BATCH_SIZE)\n",
    "\n",
    "  criterion = CrossEntropyLoss()\n",
    "\n",
    "  # Test loop\n",
    "  with torch.no_grad():\n",
    "    correct, total = 0, 0\n",
    "    test_loss = 0.0\n",
    "    for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "      x, y = batch\n",
    "\n",
    "      x, y = x.to(device), y.to(device)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "      correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "      total += len(x)\n",
    "      \n",
    "    print(f\"Test loss: {test_loss:.2f}\")\n",
    "    print(f\"Test accuracy: {correct / total * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
