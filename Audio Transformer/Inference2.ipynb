{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu126\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Description: This script trains an AST whose input has been modified to take audio insteasd of patches of images \n",
    "# Original code is based off a tutorial by Brian Pulfer\n",
    "# https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c\n",
    "# Andrei Cartera -- Mar 2025\n",
    "\n",
    "import sys\n",
    "import heapq\n",
    "import math\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as nnF\n",
    "import torchaudio\n",
    "from AudioTransformer import AudioTransformer\n",
    "from Inference_process import vectorize_f\n",
    "from Inference_process import process_repcycles as get_repcycles; \n",
    "\n",
    "np.random.seed(0) \n",
    "torch.manual_seed(0)\n",
    "\n",
    "print(torch.__version__)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop(waveform, target_length=16000):\n",
    "  if waveform.shape[1] < target_length:\n",
    "    padding = target_length - waveform.shape[1]\n",
    "    waveform = nnF.pad(waveform, (0, padding))\n",
    "  else:\n",
    "    waveform = waveform[:, :target_length]\n",
    "  return waveform\n",
    "\n",
    "def get_repcycles_wav(waveform, repcycles, vec_size=48):\n",
    "\n",
    "  # [0,0,0,0,0,(2025.2, 2125.3),(),(),0,0,0,0,0,0]\n",
    "  #2D Array of the repcycles with size of the columns being the max repcycle size\n",
    "  out = torch.zeros(len(repcycles), vec_size)\n",
    "\n",
    "  for i, cycle in enumerate(repcycles):\n",
    "    if not cycle:\n",
    "      continue\n",
    "    repc_wav = vectorize_f(waveform[0, math.floor(cycle[0]):math.floor(cycle[1])], vec_size)\n",
    "    out[i] = torch.tensor(repc_wav) \n",
    "  \n",
    "  return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from models/N_ATmodel_32SEG_48VEC_E100_8_4_B64_H32_3.pth\n",
      "zero:\t0.00\n",
      "one:\t0.00\n",
      "two:\t1.00\n",
      "three:\t0.00\n",
      "four:\t0.00\n",
      "five:\t0.00\n",
      "six:\t0.00\n",
      "seven:\t0.00\n",
      "eight:\t0.00\n",
      "nine:\t0.00\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "classes = ['zero', 'one', 'two', 'three', 'four', 'five', 'six', 'seven', 'eight', 'nine']\n",
    "NUM_CLASSES = 10\n",
    "N_SEGMENTS = 32\n",
    "REPC_VEC_SIZE = 48\n",
    "\n",
    "EPOCHS = 100\n",
    "N_HEADS = 8\n",
    "N_ENCODERS = 4\n",
    "BATCH_SIZE = 64\n",
    "HIDDEN_DIM = 32\n",
    "DROPOUT = 0.15\n",
    "ACTIVATION=\"gelu\"\n",
    "LR = 0.0009\n",
    "CONTROL = False\n",
    "\n",
    "MODEL_PATH = f'models/N_ATmodel_32SEG_48VEC_E100_8_4_B64_H32_3.pth'\n",
    "  \n",
    "model = AudioTransformer(N_SEGMENTS, REPC_VEC_SIZE, N_ENCODERS, HIDDEN_DIM, N_HEADS, NUM_CLASSES).to(device)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, weights_only=True,map_location=torch.device(device)))\n",
    "print(f\"Model loaded from {MODEL_PATH}\")\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "audio_path = Path('audio/inference/2_2.wav')\n",
    "\n",
    "#input :\n",
    "#0 0 0 0 0 ...\n",
    "#0 0 0 0 0 ...\n",
    "#0.1 0.2 0.5 0.2 ...\n",
    "#0 0 0 0 0 ...\n",
    "#0 0 0 0 0 ...\n",
    "\n",
    "waveform_t , _ = torchaudio.load(audio_path) # load the audio file, returns a tensor of shape (1, T) where T is the number of samples\n",
    "repcycles = get_repcycles(waveform_t)\n",
    "input = get_repcycles_wav(waveform_t, repcycles, vec_size=REPC_VEC_SIZE).to(device) # get the repcycles for the waveform and vectorize them\n",
    "\n",
    "# Add a batch dimension to the input tensor\n",
    "input = input.unsqueeze(0)  # Shape: (1, sequence_length, feature_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "  output = model(input)\n",
    "\n",
    "np_output = (output.cpu()).numpy()[0]\n",
    "\n",
    "for i, value in enumerate(np_output):\n",
    "  print(\"%s:\\t%.2f\" % (classes[i], value))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
